var projectsDB = [{"pitch":"Interactive wallpaper of New York city using recycled subway maps.","author":"Gabriela Gutierrez","title":"Light Up New York City","url":"http://itp.nyu.edu/~gg964/blog/archives/1483","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1334856002_popup1.png","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tThe subway map as an object of art is completely underused. I’m a big fan of transportation, maps and electronics. I try to combine all these interests in this one project to create an interactive pop-up book of NYC in which the main stations are lit up by tiny, super bright surface-mount LEDs.<br><br><br><br>\nThe idea came to me while riding the train home; it struck me that the lines and dots connecting all the subway lines resembled the layout of circuit boards. It also occurred to me that they serve sort of the same purpose: electrons travel along the connections, just as subway lines move millions of people every day, filling the city with its unique energy.<br><br><br><br>\nThis metaphor of the map as a circuit board was the point of departure for a long journey exploring paper-based electronics, soft circuits, and small blinking lights. In my first experiments, I used copper tape to literally trace a parallel subway system behind the subway map. In place of the stations, I glued a magnifying glass to a pair of helping hands and spent hours soldering tiny surface-mount LEDs directly onto the paper."},{"pitch":"This object is a light sculpture and low resolution video piece focused on capturing skylight. Light becomes a medium for emotional telepresence.","author":"Geetha Pedapati","title":"Sunlight as Mediated by an Object","url":"http://itp.nyu.edu/shows/spring2012/sunlight-as-mediated-by-an-object/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1336750047_sunlight.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tEvery morning I can watch the sunrise from my apartment window without actually seeing the sun.  In New York, it’s rare to be able to see the evening or morning sun. Instead, we watch the colors and reflections on the buildings slowly shift through the day. This piece attempts to capture and recreate these subtleties of natural daylight in a digital display. Equipped with an LED display and a wireless camera, it focuses the viewer on a displaced patch of sky. This surreal window invites quiet contemplation. It examines how light and space are perceived and seeks to find similarities, if any, between the subtleties of natural and constructed light."},{"pitch":"The Bricolo mechanical music system allows musicians, composers and DJs to incorporate musical robotics into their digital music production setups and turn any object into a computer-automated instrument.","author":"Nick Yulman","title":"Bricolo","url":"http://bricolomusic.com","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335151159_main_image_3.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tThe Bricolo mechanical music system allows digital music makers to incorporate robotics into their performance and recording setups.  The kit consists of a variety of mechanical modules that users can attach to acoustic instruments or any physical object.  A control box connects these modules to a computer or midi instrument, allowing for easy integration with popular music production platforms. The basic set include mechanisms for striking, tapping and shaking objects.  The Bricolo “Thing Synth” turns physical objects into tunable oscillators, allowing users to play melodies with them.  No custom construction or programming is required, just plug in the modules, attach them to objects and start making music.<br><br>\nBricolo emphasizes flexibility, allowing users to experiment with different sounds on the fly and make the physical manipulation of objects part of their performances.  It invites digital musicians to look up from their screens and view everything around them as a potential sound to utilize.  It encourages playful experimentation with physical materials and translates the control and precision of musical sequencing programs into the world of objects.<br>"},{"pitch":"'Just For You' is a communication device \nthat establish a link between mobile phone and a toy","author":"Ji Hyun Moon","title":"‘Just For You’, an intimate telecommunication toy","url":"http://itp.nyu.edu/shows/spring2012/just-for-you-an-intimate-telecommunication-toy/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1336752721_justforyou.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tIt is designed with children in mind. The target audience is children ages 4-7 and their parents who are geographically separated from the children.<br><br>\nParents can practice recording stories on their mobiles and when they send the recording to the toy, which their children will have, they can attach photos with 6 different backgrounds that each express a different emotion. On the other hand, children can play voice messages and replay them any time they need. Also, children can respond to their parents by pressing emotion buttons.<br><br><b>Background</b><br>http://ezinearticles.com/?Encouraging-Emotional-Expression-in-Children&amp;id=2834422<br><br>\nBy Matt Casper<br><br><br><br>\nvolunteerguide.org<br><br><br><br>\nhttp://mindinthemaking.org/article/executive_function_skills_are_essential_to_americas_present_and_future/<br><br><br><br>\nhttp://opinionator.blogs.nytimes.com/2009/01/06/you-wont-remember-this-either/<br><br><br><br>\nhttp://www.sensoryspectrumshop.com/category.php?category_id=186<br><br><br><br>\nhttp://mindinthemaking.org/article/executive_function_skills_are_essential_to_americas_present_and_future/<br><br><br><br>\nZero-to-Six-Electronic-Media-in-the-Lives-of-Infants-Toddlers-and-Preschoolers-PDF, Henry J.Kaiser Family Foundation and the Children’s Digitla Media Centers(CDMC)<br><br><br><br>\nAinsworth’s Theory and Bowlby’s Theory:<br><br>\nThe Origins of Attachment Theory By John Bowlby and Mary Ainsworth<br><br><br><br>\nHarry Harlow’s experiment<br><br><br><br>\nLess Is More: Meta-Analyses of Sensitivity and Attachment Interventions in Early Childhood by Marian J. Bakermans-Kranenburg, Marinus H. van IJzendoorn, and Femmie Juffer Leiden University<br><br><br><br>\nThe whole child development guide by LEGO group<br><br><br><br>\nInteraction Design and Children by Juan Pablo Hourcade<br><br>\nUniversity of Iowa, USA, hourcade@cs.uiowa.edu<br><br><br><br>\nhttp://developingchild.harvard.edu/resources/multimedia/videos/inbrief_series/inbrief_program_effectiveness/<br><br><br><br>\nYardsticks:Children in the Classroom, Ages 4-14/ by Chip Wood<br><br><br><br>\nTangible user interfaces for children<br><br>\nby Glenda Revelle,Sesame Workshop, New York, NY<br><br>\nOren Zuckerman, MIT Media Lab, Cambridge, MA<br><br>\nAllison Druin, University of Maryland, College Park, MD<br><br>\nMark Bolas, USC School of Cinema-Television, Los Angeles, CA<br><br><b>Audience</b><br>\n          \t\t\t\tMainly 4-7 years old children and their parents. <br>\nfurthermore, this will be very helpful for autistic children to develop social interaction skills<br><br><br><br><b>User Scenario</b><br>\n          \t\t\t\t1. Imagine a child left alone at home while her parents are at work.<br>\n2. Imagine a passive and negatively-minded child, facing difficulties in integrating with her peer group.<br>\n3. Imagine a child undergoing extensive inpatient care, unable to leave the hospital and separated from his parents.<br>\n4. Or imagine yourself, wanting to delve into childhood memories of your parents or grandparents. The warmth you feel as you play back a voice message left by your late grandmother.<br><br><br><b>Implementation</b><br>\n          \t\t\t\tMobile application<br><br>\nThe ‘Just for you mobile App’ allows geographically distant or frequently absent parents to use media devices that they already use in their everyday lives -- which provides ease of access -- to record bedtime stories for their beloved children and to communicate with the Just for you toy that their children have. Through the the app, parents can send their children brief messages of love, or practice and record lullabies and bedtime stories. Parents can also attach photos of their own faces to the messages, expressing one of six different emotional states: Happy, Confident, Sad, Angry, Lonely, and Surprised.<br><br><br><br><br><br>\nThe ‘Just for you’ toy<br><br>\nFollowing a children-oriented design scheme, this toy is a cushion-type huggable doll with a digital display screen. It also has six facial expression buttons that children can press to convey their emotions and reactions. The content consists of songs and stories that parents record for their children with the mobile app 'Just for You'.<br><br>\nThe device makes use of existing APIs such as PhoneGap, and the Arduino Prototyping API for Wifi, MP3, and an LCD touch screen to apply the technology of digital communication for children.<br><br>\n In turn, the children can download voice messages to the toy when Wifi is available, and then listen to the stored files offline anytime they need. In contemporary life, children six and under are observed to spend three times longer with screen media every day than they do reading or being read to. Therefore, the ‘Just for you’ toy is helpful for maintaining children’s close relationships with their parents and for their early education.<br><br><br><br><br><br>\nAppearance<br><br>\nThe matter of which colors and design motifs boys and girls prefer has been surveyed and applied to the device. Children in general like bright and cheerful colors which are highly saturated, and an appropriate number of colors are used on the toy so that it feels comfortable and friendly. The toy body is filled with a soft and squishy cushion and covered in a non-toxic, eco-friendly 100% polyester micro-weave fabric for young children’s sensitive skin. It is created without pricey media components, in order to make it affordable for many users who are willing to buy and use it to express their love for their children.\t\t      \t\t<br><br><b>Conclusion</b><br>\n          \t\t\t\tListen and watch<br>\nAll parents need to take the time to listen, observe, and talk to their children about what is happening around them. This can teach children good listening and communication skills, respect and support for differing opinions, and ways to manage fears and anxieties.<br>"},{"pitch":"GoogleBooth is a human-powered search engine.","author":"Guilherme Costa, Stefanie Kleinman","title":"GoogleBooth","url":"http://itp.nyu.edu/shows/spring2012/googlebooth/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335147417_photon2n.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tGoogleBooth - Guilherme Pena Costa &amp; Stefanie Kleinman<br>\nConnect to your search. Trade instant gratification for investigation. <br><br>\nThis piece invites users to reconsider and recontextualize the familiar experience of using an internet search engine. Raising questions of trust, privacy, impatience, addiction, and authority, the piece aims to inspire users to make a more personal, intimate connection to the process of seeking, providing, and discovering information. <br><br>\nGoogleBooth represents the danger of losing sight of the legacy and context of learning in favor of a quick fix. The notion of \"searching\" for information implies a dynamic process. In a world of immediate answers, the beauty of that process is nearly lost. GoogleBooth aims to recall earlier methods of learning and draw attention to the beauty of participating in a search. The installation serves to encourage people to be more aware and critical of how search engine results are generated and the dangers of becoming too dependent on such technologies.<br><br><b>Audience</b><br>\n          \t\t\t\tEverybody is welcome and encouraged to use our GoogleBooth. The success of the booth depends on people participating and using the booth both to receive and provide answers to search questions. Allowing many users to take time to explore their search, investigate the questions they have, offer advice and insights, and spend time inside the booth is crucial to the concept of the booth.\t\t\t\t\t<br><br><b>User Scenario</b><br>\n          \t\t\t\tTo Use GoogleBooth:<br><br>\n1. Write Your Search Query On a GoogleBooth Search Card<br><br>\n2. Drop Your Search Card in the \"Search\" Slot on the front of the GoogleBooth<br><br>\n3. Wait to receive the results to your search. You will receive the card back through the \"Results\" slot. It may take seconds or hours. <br><br>\n4. After you receive your results, please feel free to go inside GoogleBooth to explore.\t\t\t\t\t<br><br><b>Implementation</b><br>\n          \t\t\t\tGoogle Booth is an interactive installation made of wood, vinyl stickers, index cards, encyclopedia pages, book pages, and article clippings. Inside the booth, we have installed a light and several wood shelves as well.\t\t      \t\t<br><br><b>Conclusion</b><br>\n          \t\t\t\tWe enjoyed this project, as we learned a lot about the reasons people search for information and the types of information they search for. We enjoy encouraging people to use the booth and explore the interior, as the experience is unique for all users. We had fun creating a special and educational environment that is simultaneously similar and very different from the experience of an online Google search. We intentionally chose a popular and widely used technology for this project so that people would immediately be able to identify with it and be interested in exploring it."},{"pitch":"Information analytics gives us the tools to see inside large datasets, revealing patterns, trends, and hidden insight. Shouldn’t we have the same tools to explore the most important data in our lives—our personal data?","author":"Alex Dodge","title":"Kioku","url":"http://alexdodge.com/kioku/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335158230_kioku.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tTaking personal photos has never been easier and because of it our personal photo albums continue to grow at a staggering rate. Kioku offers a new way of exploring our memories using powerful analytical tools. Kioku allows users to see their photos in ways they never have before, giving them the ability to explore their personal photo collections in many dimensions simultaneously. Have you ever wondered what every birthday from the past five years looked like sorted by your best friends, number of ”likes”, and color? Now you can find out.  Life isn’t a timeline. Let your memories come alive.<br><br>\nKioku was possible with the help of Akira Shibata, Andrew Childs, and many others.<br>"},{"pitch":"The Burritob0t™ 3D prints an edible extrusion of configurable Mexican food employing a hybrid of digital fabrication and molecular gastronomy - call it digital gastronomy for the hungry masses.","author":"Marko Manriquez","title":"Burritob0t","url":"http://www.burritob0t.net/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1334980112_burritobot.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tBurritob0t.net is a platform for rapid prototyping and tracing the source of food in our lives to reveal hidden issues revolving around fast food: labor practices; environmental consequences; nutritional values. Mexican fast food is emblematic of the assembly line, mass produced era of modern consumables – appropriating the authenticity of the ethnic food sensibility it purports to embody while masquerading as an edible like substance.  Because the burrito is a mass market consumable, it lends easily as a way for examining and stimulating discussion on various aspects of the food industry including: how and where our food is grown, methods of production, environmental impact, cultural appropriation and perhaps most importantly - what our food means to us. By parodying the humble burrito’s ingredients and methods of production we can shed light on these exogenous factors and interconnected systems surrounding the simple burrito."},{"pitch":"Genesis is a retro-futuristic virtual pet adventure game where players raise creatures to help them reconstruct their world.","author":"Liza Singer","title":"Genesis","url":"http://lizasinger.net/genesis/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335174135_posterresized.png","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tWhat happens when you give a child power over another being? What happens when you introduce them to death? What are the consequences of genetic engineering? Drastic cosmetic surgeries? Cheating death? What happens in a world where this is the normal? What kinds of personalities would accept this? Genesis' narrative and gameplay deconstruct the virtual pet adventure genre, exploring what it means to give life to an AI creature and the psychological impact on these creatures when they are used as toys or tools. Genesis subverts and pokes fun at the stark black and white moral choices found in most contemporary games, offering more realistic and ambiguous dilemmas. Common game tropes are tested, cheating is encouraged, and consequences are enacted at every turn. Set in a post-apocalyptic world, players are given AI creatures called chimera to raise, teach and train, to aid in reconstructing their world. This creates a unique and personal narrative experience as each environment adapts to the player's choices. Chimera respond to and learn from these choices, mirroring the player's true nature.<br><br><b>Background</b><br>While fantastic, there will also be a level of research going into more in-depth child psychology. Beyond my fascination with neuroscience, I’m particularly interested in developing creatures with personalities that feel organic and sincere based on players action. They are meant to evolve uniquely and organically, and any evolution that seems expected and played out will hurt the intention of the game. So when I speak of morality in Genesis I’m not speaking in terms of the morality that feels familiar in most contemporary games. While I adore how far games have come, I still feel that there is much to be gained in the ways of a narrative.<br><br><br><b>Implementation</b><br>\n          \t\t\t\tA refined proof of concept with working prototypes. Due to the sheer mass of the game, my goal is to develop a cohesive proposal, a built-out bible of the narrative and it’s inner workings and a strong simulation of what the full game might feel like. To fully flesh it out, I would prospectively need a team to assist me in completing it. Being able to leave ITP with a strong business plan and a intelligent marketing strategy would be ideal."},{"pitch":"The exploration of mobile technology, social networking and traditional beauty culture","author":"Rong Yong","title":"MOMO-a Mobile App to Share Beauty Experience","url":"http://momo-2.com","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335190905_momo_rongyong.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tMOMO is the magic box that helps you (specifically Chinese girls) to record all using experience with beauty products. You can easily add photo, info, description, geo-location, rate and comment for these products, and organize them into four steps. That is, Wishlist (products you want buy), Mybox (products you own), Routine (products you are using) and Empty (products you used up).<br><br><br><br>\nMOMO is also a private and light community that helps you to share and learn with each other. You can review and comments on your friends’ beauty products, keep track of all updates in your circle, build the friendship with ones have the same desire to pursue beauty. You still have the chance to send your recommendation to your public social networks like Facebook, Twitter and Weibo."},{"pitch":"A bombastic multi-media drum-powered experience","author":"Alex Samoilescu, Mark Kleback, Seyyed Ali Sajjadi","title":"Drummageddon","url":"http://itp.nyu.edu/shows/spring2012/drummageddon/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1336623989_drummageddon.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tA drumset is equipped with pressure sensors, and when the performer plays the kit, each drum triggers a different video or lighting effect. There will be 3-4 drum sensors, 1-2 variable pedals, a DMX lighting rig, a projector, and a Kinect. The idea is that media in a performance space should be a factor of the performers and the volume of the audio<br><br><b>Background</b><br>The Arduino is the ideal input device, and since it supports up to 6 inputs, I can use multiple drum inputs, as well as a few variable foot pedals for other effects. I'd love for the drummer to be able to control everything easily, without stopping a performance. The program Max/MSP is the obvious choice for software because of its versatility to manipulate live input data.<br><br><b>Audience</b><br>\n          \t\t\t\tThe first target would be active drummers who could incorporate this into a live performance. Alternately, the audience is the target for the effects themselves.\t\t\t\t\t<br><br><b>User Scenario</b><br>\n          \t\t\t\tA person sits down at the drumset. Whether or not they know how to play the drums, they understand that each drum triggers a separate lighting/visual effect. They experiment for 3-5 minutes playing on the kit to see each effect, then hand off the drumsticks to the next performer.\t\t\t\t\t<br><br><b>Implementation</b><br>\n          \t\t\t\tThere will be a bass drum, floor tom, and snare drum each with piezo triggers built-in. There will also be a Roland digital hi-hat trigger which will act as a fourth input trigger. The remaining 2 inputs will be variable pedals that will adjust fade length/brightness/etc. Additionally, there will be a Kinect mounted in front of the kit controlling the projection effects. These will be projected directly in front of the kit so the performer can see exactly what is happening. The DMX lights will be mounted either above or below the kit. There will be one computer, and 2-3 Arduinos"},{"pitch":"A web-based word game, designed specifically for people suffering from early-stage memory loss, that aims to engage users with a mentally stimulating, entertaining, and streamlined digital experience.","author":"Craig Protzel","title":"Story Scramble","url":"http://itp.nyu.edu/shows/spring2012/story-scramble/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335205628_thesis_storyscramble_web.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\t\"Story Scramble\" is an online brain-exercise game created for people with early-stage memory loss, specifically people with Mild Cognitive Impairment (MCI) and Early-Stage Alzheimer's. The game challenges its users to unscramble words from the headlines of current news articles. After successfully unscrambling the word (or words), a user can choose to read the article or continue playing the game. Very few internet applications currently exist that are manageable and enjoyable for people with dementia. By being easy to navigate, providing cognitive stimulation, and focusing on engaging and relevant content, \"Story Scramble\" fosters a much-needed positive experience for this demographic. Developed in conjunction with the NY Alzheimer's Association, \"Story Scramble\" can be used both in group settings or individually at the institute's early-stage memory loss support center, which will be opening in NYC Spring 2013."},{"pitch":"The descriptive camera outputs a text description of the captured scene instead of a photograph.","author":"Matt Richardson","title":"Descriptive Camera","url":"http://mattrichardson.com/Descriptive-Camera/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335112491_descriptive-camera-320px.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tThe Descriptive Camera works a lot like a regular camera—point it at subject and press the shutter button to capture the scene. However, instead of producing an image, this prototype outputs a text description of the scene. Modern digital cameras capture gobs of parsable metadata about photos such as the camera's settings, the location of the photo, the date, and time, but they don't output any information about the content of the photo. The Descriptive Camera only outputs the metadata about the content.<br><br><br><br>\nAs we amass an incredible amount of photos, it becomes increasingly difficult to manage our collections. Imagine if descriptive metadata about each photo could be appended to the image on the fly—information about who is in each photo, what they're doing, and their environment could become incredibly useful in being able to search, filter, and cross-reference our photo collections. Of course, we don't yet have the technology that makes this a practical proposition, but the Descriptive Camera explores these possibilities.<br><br><b>Background</b><br>After the class readings and discussions about parsability of space, most of the research went into effectively using the BeagleBone and Amazon's Mechanical Turk Service.<br><br><b>Audience</b><br>\n          \t\t\t\tThe audience for this prototype is those who appreciate novel uses of technology. As for a proper application of the technology, I think it's possible that similar methods could be used to help us search, filter, and cross-reference our photo collections.\t\t\t\t\t<br><br><b>User Scenario</b><br>\n          \t\t\t\tTo demonstrate the prototype during an exhibition, I would have the camera strapped around my neck. When someone approaches, I would very briefly explain what it is and ask if they want their picture taken. I would take their picture (or a picture of something in the room) and explain the technology behind it as we wait for the print to come out. I would let the person keep their print.\t\t\t\t\t<br><br><b>Implementation</b><br>\n          \t\t\t\tAt the core of the Descriptive Camera is a BeagleBone, which connects to the internet via Ethernet. A USB webcam attached to the BeagleBone captures the image when the shutter is pressed and the image is uploaded for processing by Amazon's Mechanical Turk service (or in accomplice mode, it IM's a link to picture to someone I predetermine). When the camera receives a response from the server, it prints it out with a built-in thermal printer.\t\t      \t\t<br><br><b>Conclusion</b><br>\n          \t\t\t\tI learned an incredible amount about working with the BeagleBone. Just getting a JPG from the webcam in Linux proved to be difficult. After trying lots of methods, I finally found one that worked. This was also my first opportunity working with Python's serial modules, which worked well for me. Formatting the text for the print solidified my text processing skills in Python. After getting to understand the way Mechanical Turk works (and how it's not always the best option when I want lots of results quickly), I turned to my own solution. I created my own interface for workers to submit descriptions and implemented a system to instant message them as soon as a photo is ready to be described."},{"pitch":"\"Cocooning” is a series of body-objects designed to allure into calmness.\nWhen sensing restlessness, tension or anxiety they reply with soothing vibration sound-waves and promote a return to the inner self and to restorative metamorphosis.","author":"Filipa Tomaz","title":"“Cocooning”","url":"http://itp.nyu.edu/shows/spring2012/cocooning/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335298918_cocooning_projectdatabase_image_sm.png","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tThree body-objects respond to restlessness, tension and anxiety.<br>\nAn egg shaped chair/bed senses movement while rocking; a womb like underwater sound follows the movement to stillness. <br>\nAn interlaced long pillow reads muscle tension; it softly vibrates into Om where it supports the neck, sacrum and legs.<br>\nA rounded pillow measures heart rate; slowly a vibration motion and sound wave mimick the heart beat to calmness.<br><br>\n“Cocooning” is an exercise in affective technology and seamless body-object interaction. Its aim is to conduce the body into a state of rest and promote wellness. Under the premise that the body learns viscerally, these body-objects are behavioural change companions."},{"pitch":"Suspend and Expand is an installation that allows users to interact with real and virtual 3D audio environments over advanced headphone reproduction.","author":"Michael Rosen","title":"Suspend and Expand","url":"http://itp.nyu.edu/shows/spring2012/suspend-and-expand/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1336753718_suspend.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tSuspend and Expand is an immersive surround sound installation that allows users to intuitively explore a collection of sound environments. In this personal installation experience, the user is immersed in a seamlessly changing sound world. The sounds were recorded using a specialized microphone and decoding process that enables one to experience a shifting perspective of the environment. This version of Suspend and Expand focuses on reproducing the experience over headphone reproduction, using the Kinect to change sound perspective according to the user's head rotation."},{"pitch":"\"Futile Type\" is an interactive video installation which combines Adruino, Processing and Max/MSP Jitter.","author":"Alessandra Villaamil, Ioni Gkliati, Yonatan Ben-Simhon","title":"Futile Type","url":"http://ionigkliati.com/type","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335206314_ftype.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tFutile Type is an experiential video installation that challenges the typewriter as an efficient means of textual communication. Each click of the keyboard corresponds to a blotch of ink in the video. This project was created at the Interactive Telecommunications Program (ITP) in the Tisch School of the Arts at NYU by Ioni Gkliati, Alessandra Villaamil and Yonatan Ben-Simhon."},{"pitch":"Group screening of the Advanced Animation Studio course 2012","author":"Arielle Baio, Cyrus von Hochstetter, Eric Hagan, Kate Watson, Luisa Covaria, Matthew Tennie, Miguel Bermudez, MV Carbon, Olya Mikhaliova, Saraswathi Subbaraman, Tali Blankfeld","title":"Advanced Animation Studio Screening","url":"http://itp.nyu.edu/varwiki/ClassWork/AdvancedAnimationStudio-Syllabus","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335386829_animation.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tLinear Animation Shorts and Generative Animation pieces of all students from the Advanced Animation Studio class 2012<br><br><b>Audience</b><br>\n          \t\t\t\tAll Visitors\t\t\t\t\t<br><br><b>User Scenario</b><br>\n          \t\t\t\tUser watches all animation projects running in one loop, head phones provide audio"},{"pitch":"An HTML5 web app designed for users to participate in the stories of political cartoons.","author":"Arielle Baio","title":"Bang! Political Cartooning","url":"http://itp.nyu.edu/shows/spring2012/bang-political-cartooning/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335818226_database.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tBang! Political Cartooning rethinks the way that political commentary and cartoons are shared on the web. It allows users to participate in the creation of a story or the reveal of an idea.<br><br><b>Background</b><br>I've spent a lot of time on Aljazeera, the New York Times, and various other sources on the internet researching the effects of UAV's in US foreign policy. I'm also an editor at ABC News which has featured the misuse of Drones in several features. Additionally, I've  spent time on the sites of Independent Contractors that create several drone models for the US government. This helped to contextualize the history of UAV's, how they were developed and what they are capable of today.<br><br><b>Implementation</b><br>\n          \t\t\t\tI intend to create a web application that allows that allows users to participate in political cartooning."},{"pitch":"","author":"William Jennings","title":"Born between the lines","url":"http://itp.nyu.edu/shows/spring2012/born-between-the-lines/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335486771_hugclossus_.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tBorn Between The Lines is an exploratory tool that translates email into animal glyphs.<br><br><b>Background</b><br>http://www.turbulence.org/Works/apartment/ how words fit into the context on an apartment and building said apartment. <br><br><br>\nhttp://www.genevievegauckler.com/ illustration and character design<br><br><br>\nhttp://rgm2.lab.nig.ac.jp/RGM2/func.php?rd_id=aplpack:faces chernoff faces for expression of data <br><br><br>\nhttp://www.troyabbott.com/artists/EnriqueGomez/gallery/index.html expression as a type of chimera<br><br><br>\nhttp://www.paulkasmingallery.com/artists/walton-ford/2 animals playing out historical events in a natural state<br><br><br>\nhttp://thesi.tumblr.com/post/17341475524 humungoulous a one to one relationship between nervous system response and visual cues. <br><br><br><b>Implementation</b><br>\n          \t\t\t\tA browser plugin to look back on the evolution of your emails and a \"live mode\" to watch how the emails you are writing change based on how you say something."},{"pitch":"Build SMS application without knowing how to code and understanding networks","author":"Federico Zannier","title":"SMileS","url":"http://www.riguardo.me/~web/sinatra/smsapp/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335464669_smiles.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tA doctor in Ghana can easily setup a SMS service that automatically sends SMS to his patient remembering to take the pill. <br>\nThe platform is open source and a community of developers creates blocks through an API suite that standard users can use."},{"pitch":"Written World is a communal free-form text world, overlaid on a street map of the user's physical location. It incorporates elements of online chat and graffiti to explore and create a bridge between the physical and virtual.","author":"Zach Schwartz","title":"Written World","url":"http://itp.nyu.edu/shows/spring2012/written-world/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335474017_small.png","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tWritten World is a communal free-form text world, overlaid on a street map of the user's physical location. It incorporates elements of online chat and graffiti to explore and create a bridge between the physical and virtual.<br><br><b>Background</b><br>-<br><br><b>User Scenario</b><br>\n          \t\t\t\tA visitor to http://writtenworld.org is greeted by a map of their location with fixed-width type of what users before them have written, as well as the real-time cursors of other users who are present. Users can set the color of their type, change what others have written, or go explore the rest of the world."},{"pitch":"A Selection of works from the Spring 2012 Da Vinci Code Class, taught by Yael Kanarek","author":"Amik Ahmad, Angela Bond, Douglas Thistlethwaite, Eric Hagan, Ingrid Gabor, Ioni Gkliati, Justin Lange, Mick Hondlik, Robin Reid, Rose Schlossberg, Suzanne Kirkpatrick","title":"The Da Vinci Code Class","url":"http://itp.nyu.edu/shows/spring2012/the-da-vinci-code-class/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335474282_davinci.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tOver the course of the semester students engaged with the works of Leonardo Da Vinci as a jumping off point for learning to draw with a nib and ink. The works are done in the style of Leonardo Da Vinci's codices and reference technology – old, new and future – as well as contemporary artists' practices."},{"pitch":"Fostering animals plays a critical role in lowering euthanasia rates. Fostr.org aims to educate and recruit New Yorkers to save animals and give them temporary living situations.","author":"Natalie Be'er","title":"Fostr.org","url":"http://itp.nyu.edu/shows/spring2012/fostr-org/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335200327_project_db.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tFostr.org fills a crucial gap in New York City’s animal adoption ecosytem, providing a temporary home for animals while they wait for adoption or a space in a shelter. Too often, shelters run out of space for homeless animals and have no choice but to turn them away. In too many cases, this leads to euthanasia. <br><br><br><br>\nBy creating an online tool that encourages education and recruitment, Fostr.org fills an essential space for people who cannot adopt but still want the joy of having an animal in their lives. Designed first for New York City residents, fostr.org will grow to partner with organizations in cities across the US.<br><br><br><br>\nJoin the foster revolution today! Apply at fostr.org"},{"pitch":"Financial Landscapes - Dow Jones, 2000-2012 is a visualization of the 30 stocks on the Dow Jones as an abstract three dimensional landscape.","author":"Genevieve Hoffman","title":"Financial Landscapes – Dow Jones, 2000-2012","url":"http://itp.nyu.edu/shows/spring2012/financial-landscapes-dow-jones-2000-2012/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335551427_dowjones2_sm.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tFinancial Landscapes is a series of sculptures that visualizes financial data in a physical form. By plotting the prices and volumes of shares of each stock traded since 2000, I was able to generate 3D forms that resemble an abstracted mountain range. Each stock tile is textured with its logo, which is skewed according to its 3D geometry. From the side it might not be immediately clear that the landscape is based on stock data, but a view from above gives a more intelligible picture.<br><br>\nMy goal with this project was to make physical aspects of the financial system that tend to be abstract and intangible. I want to call attention to the connection between these stocks and the environment. Not just in terms of ways that company's activities alter the environment, but how our financial system in general is in some ways an environment of its own, which shapes and is shaped by our collective activities."},{"pitch":"wearable sculpture that captures the weight of voice","author":"Ivana Basic, Yin Liu","title":"Expanded","url":"http://itp.nyu.edu/shows/spring2012/expanded/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1336743820_expander.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tThis is a wearable sculpture that is positioned over the head of the user. When the user makes a constant sound, water starts pumping into the bottom of the sculpture thereby increasing the weight of the sculpture. Once the sound stops, the weight of the sculpture gradually decreases as the water slowly pumps back out. Meanwhile, the voice will be played back through speakers on the floor.<br><br><b>Background</b><br>In modern urban cities, people are immersed in their busy city lives and often neglect the simple but powerful physical presence of their own body and voice.  This piece is installed in a quiet space where people are invited to experience the piece in peace and quiet.<br><br><b>Audience</b><br>\n          \t\t\t\teveryone"},{"pitch":"Bocce Draw is a project using projection and computer vision to add interactivity to the game play of Bocce Ball.","author":"Alex Samoilescu, Danne Woo","title":"Bocce Draw","url":"http://itp.nyu.edu/~as6319/blog/spatial-media-bocce-draw-presentation/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335644716_bocce.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tBocce Draw is a project using projection and computer vision to add interactivity to the game play of Bocce Ball.   Using the Sony PS3 camera, distance for each bocce ball from the Pallino (white ball) is measured.  A projector is used to display circular patterns onto the court.  To indicate which ball is closest, the projected circle pulses."},{"pitch":"\"Sluts Across America\" is a participatory Internet project designed to highlight the absurdity of the current war on contraception and reproductive health being waged by conservative members of American society.","author":"Roopa Vasudevan","title":"Sluts Across America: A Birth Control Advocacy Project","url":"http://www.slutsacrossamerica.org","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335303325_sluts_day2.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\t\"Sluts Across America\" is an Internet project designed to highlight the absurdity of the current \"war on women\" being waged by the GOP and other conservative members of American society.  The homepage is a large map of the United States, littered with markers indicating the locations of women and men who self-identify as supporters of birth control.  Submitters call themselves \"sluts\" in an effort to reclaim this derogatory name and classification from those who use it to assassinate the character of the people who choose to protect themselves and be responsible about sexual health.  Site users across the country can upload their stories (including recording a video into the browser if they choose) and automatically have their markers included on the map.<br><br><b>Background</b><br>The past year's firestorm of controversy over birth control, specifically brought along by both the threat to defund Planned Parenthood as well as Obama's mandate that birth control be covered by insurance without copays, has struck me as both troubling and idiotic.  In my mind, and the minds of so many others I know, birth control is not a luxury -- it is a right, and using contraception indicates that you have an awareness of what is responsible behavior when it comes to sexual health.  I wanted to create a project that could visually illustrate the sheer number of supporters of reproductive rights in the US, show that they are everywhere and not just in certain liberal pockets of the country, and at the same time illustrate just how absurd this entire battle over contraception is when so many people (from all different walks of life) rely on it every day.<br><br><b>Audience</b><br>\n          \t\t\t\tThe target audience is anyone and everyone who has ever used birth control, relied on birth control, or stood behind those who depend on birth control.\t\t\t\t\t<br><br><b>User Scenario</b><br>\n          \t\t\t\tA user who visits the site immediately sees the map interface, with \"slut\" markers on it.  By clicking on the icons, the user can read all of the statements that others have submitted about why they are \"sluts,\" or why they use or support birth control.  The home page also has a form on it where the user can enter their own reason and zip code; when they press submit, the map refreshes, with the user's marker added.  There are also sections for video testimonials, where a user is invited to watch the videos others have recorded as well as record one him/herself, and \"about\" and \"contact\" sections that the user can explore if he/she wants more information.\t\t\t\t\t<br><br><b>Implementation</b><br>\n          \t\t\t\tThe app is written in Ruby/Sinatra, uses the Google Maps API, and is running on Heroku.  I have been manually maintaining the database since launch to make sure that the site doesn't get overwhelmed by spam and to make sure that as many of the stories that people have submitted will be seen as possible.  The site went viral within 48 hours of official launch and gathered over 7,000 submissions between April 23 and April 27, 2012.\t\t      \t\t<br><br><b>Conclusion</b><br>\n          \t\t\t\tI learned how fulfilling web development can be, and also how easy it is to get sucked into a vortex of code that becomes extremely difficult to get out of.  I learned that web dev is never done -- there's always something you can be fixing, all the time.  And I learned how a simple interface paired with a simple submission process can become a catalyst for an outpouring of participation, engagement, and support for and from thousands of people."},{"pitch":"Rºad’io’’ is an interactive installation that allows the user to explore the American cultural landscape by tuning into the various radio stations playing around the country.","author":"Lia Martinez, Mercedes Blasco","title":"Rºad’io’’","url":"http://half-half.es/interactive-design/Radio/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335661258_radio_top.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tThe user navigates a radio knob around the surface of a wooden map to listen to what is broadcasting in that area."},{"pitch":"Sing Whale is an iPad storybook where small children can make friends with a whale -- by speaking (or singing!) in his language.","author":"Lia Martinez","title":"Sing Whale","url":"http://sing-whale.com","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335661539_thesisbook_lia320-01.png","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tChildren can sing in “whale” by following visual guides and singing accordingly -- kind of like tonal connect-the-dots or language-learning karaoke. <br><br>\nThe songs that you learn are simple, things you might ask someone that you just met, like: “What’s your favorite color?” (purple). The storybook revolves around one particular whale, and he’s got personality. He’s afraid of sharks, never far from his mom, likes tacos… actually shrimp tacos -- actually without the tacos -- okay, just shrimp. Sing-Whale gives families a chance to be silly. <br><br>\nWhile scientists in the real world don’t yet know how to decode whale songs, in this make-believe one we find that if we can speak their language, we can be friends."},{"pitch":"Real time scientific data acquisition, visualization, and sonification for makers, artists, citizen scientists, and other curious types via an exploration of space from Earth.","author":"Jennifer Shannon","title":"Listening to the Sun","url":"http://www.listeningtothesun.com","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335458872_listeningtothesunpresentationtitle.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tListening to the Sun is an experiment in making hands-on science accessible to curious types through affordable DIY technologies. By repurposing easily found and/or discarded consumer electronics, it is possible to create tools that enable the exploration of our atmosphere and outer space without leaving Earth. Coupling these tools with computers and open source software and hardware allows for real time data acquisition, visualization, and sonification. Listening to the Sun is a two part endeavor: a website and a visualization piece. The website, www.listeningtothesun.com, is an information portal containing detailed build instructions and tutorials for DIY radio astronomy projects, accompanied by brief explanations of some of the science behind the technology and links to further resources. Aurora is a real time visualization of the invisible electromagnetic ballet of NYC made to mimic the Aurora Borealis."},{"pitch":"A computer’s pixelated collection: exploring the vertiginous implications and potential beauty of collecting in the digital realm.","author":"Anna Pinkas","title":"Collectio ex machina","url":"http://itp.nyu.edu/shows/spring2012/collectio-ex-machina/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1334803899_projectdatabase.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tA collection can be defined as the gathering, organizing and displaying of items - three verbs one could easily use to describe a computer’s modus operandi. \"Collectio ex machine\" was inspired by the parallels between the human impulse to collect and computational data collection. The subject of this screen-based artwork is the layered collection of an anthropomorphized computer. Pixels are the only entities the machine can display and are thus at the core of this endeavor: the algorithm retrieves an image titled “my collection” from flickr and divides it into 4x4 sequences of pixels. Each of theses 16-pixel arrays is used to generate a stack of 100 images (all containing this particular sequence). The collection is ongoing. The process is repeated over and over again. This absurd combination of arbitrary decisions and strict rules produces a mesmerizing, and dizzying, collection - an uncanny evocation of our own obsessions.<br><br><b>User Scenario</b><br>\n          \t\t\t\tThe viewer can peruse the collection through keyboard/mouse triggers. There are different levels for him/her top explore: 1. The master image 2. The master image's arrays of pixels 3. A particular array's stack of images 4. A particular image within a stack. The \"collecting\" algorithm (a processing sketch retrieving images from flickr) is presented alongside the main visualization. It offers insight into the computer's collecting impulse and highlights the ongoing, time-based nature of the project."},{"pitch":"If you could speak to 1 million people, what would you say?","author":"Alvin Chang, Greg Dorsainville, Josh Begley, Yoonjo Choi, Zena Koo","title":"thelistserve","url":"http://http://thelistserve.com/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335742008_thelistserve.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tThis is an e-mail lottery. One person a day wins a chance to write to the growing list of subscribers. It could be you."},{"pitch":"A mobile-phone based camera trap for animal studies, enabling remote access to the photographs, embedded geolocation data and customizable features to suit various research goals.","author":"Christie Leece, Mark Breneman, Michael Uzzi, Ryan Viglizzo","title":"Nature Calls","url":"http://itp.nyu.edu/shows/spring2012/nature-calls/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335754011_landscape.png","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tNature Calls is a mobile phone-based camera trap for animal behavioral studies. With researcher workflow and harsh climates in mind, we designed a camera trap that transfers photographs remotely, is completely compartmentalized for easy maintenance, is entirely waterproof, and is minimally disruptive to wildlife. The trap is distinct from off-the-shelf models in that it uses a mobile phone, opening up the world of real-time data analysis, remote data transfer, location stamping, and a host of customizable features for various research objectives.<br><br><b>Audience</b><br>\n          \t\t\t\tField researchers, animal behavior enthusiasts\t\t\t\t\t<br><br><b>User Scenario</b><br>\n          \t\t\t\tThe camera, sensor, and flash are mounted to a tree and the researcher uses the touch screen interface to start the application. The arduino and battery housings sit at the base of the tree. When the PIR sensor is triggered, the phone takes a picture. When days have passed, the researcher will go out to gather the images. Since there is no cell/wifi in the jungle, the researcher will go to the phone, turn on bluetooth on a separate handheld device, and download the images from the phone without breaking open the waterproof case. If the battery needs changing, they can also do that it without moving the camera or any related sensors or arduino."},{"pitch":"A small video installation of a man eating in a cellar","author":"Veronika Dubrovskaya","title":"countryside","url":"http://itp.nyu.edu/shows/spring2012/countryside/","image":"images/none.png","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tIt is a miniature of a cellar created within 2 feet by 2 feet box .  The rare projection on the room projects a video of a man eating within this room, - it appears as if you are looking inside his cellar."},{"pitch":"Call the provided phone number to be a part of a projection-mapped Grid Based Audio Sequencer","author":"Byung Han Lim, Dong Ik Shin","title":"Call Your Sequencer","url":"http://itp.nyu.edu/shows/spring2012/call-your-sequencer/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335758222_popo.png","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tThis is a project combining /Redial + Cones and Box(Jitter)/ we wanted to create a audio sequencer which let anyone who has cellphone to participate a collaborative real-time composition where the user-input generated graphics will be mapped to a large wall in a very simple yet visually fun grid-based 3D cubes"},{"pitch":"Machine dreaming infinite State is a generative art sculture that prints out generative algorithmic poems.","author":"Owen Roberts","title":"Machine Dreaming Infinite State","url":"http://itp.nyu.edu/shows/spring2012/machine-dreaming-infinite-state/","image":"images/none.png","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tMachine dreaming infinite state is a sculture of human head that prints out generative algorithmic poems(written by me) through her scull. The poems are written as algorithms, instructions, or examples designed to lead the audience into a journey to the unreachable state of infinity. The poems attempt to combine the past, the present, and the future as one.<br>"},{"pitch":"'Bouncing Balls: A Collective Experience of Random and Analog Commands (documented video)' is an audience-participating performance experience which interprets a digital processing sketch into an analog version.","author":"Hanna Kang-Brown, Jee Won Kim","title":"Bouncing Balls","url":"http://vimeo.com/41268462","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335801532_balls.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tThe planning of performance started with interpreting a basic Processing sketch that digitally bounces balls with random option and commands. With this particular Processing sketch, one more balls in generated when mouse is pressed, all balls could be cleared out when space-bar is pressed, and the color of balls change when character 'T' is pressed. The performance itself tried to give participants the most subtle instructions in the beginning and also balls and the command center. The command center is consisted of three analog commands- a bag of pretzels, a pillow, and a maraca. These represent the interpreted commands that make our bodies to generated movement in the space."},{"pitch":"REMQuilt helps couples decipher the secret language of their sleep. Each night, it tracks their movement, position, and body contact; and in the morning, it weaves this data into a representation of restlessness, proximity, and touch that is projected on their bed in a quilt-like pattern.","author":"Amelia Hancock, Justin Lange, Patrick Muth","title":"REMQuilt","url":"http://itp.nyu.edu/shows/spring2012/remquilt/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1336599158_remquilt_for_show.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tREMQuilt helps couples decipher the secret language of their sleep. Each night, it tracks their movement, position, and body contact; and in the morning, it weaves this data into a representation of restlessness, proximity, and touch that is projected on their bed in a quilt-like pattern.<br><br><br>\nREMQuilt is meant for a limited audience-- the visualization is intended for couples. Because of the intimacy associated with the context of this data, we chose an abstract graphic approach-- both to mimic the abstraction of traditional quilt squares, as well as to respect to the couple's shared secret data.<br><br>\nWhen viewed from the side of the bed, rows of quilt squares read left-to-right, creating a grid that is representational of a possible nine hours of sleep. Each individual 'quilt square' itself represents two-and-a-half minutes, accounting for each partner's time, creating an x axis of 24 quilt squares (or an hour of sleep). The y-axis accounts for nine hours for each couple, represented by a total of 18 quilt squares.<br><br>\nThe data is represented graphically in both geometric shape and color. To represent movement, we have chosen two horizontal 'jagged' lines, that, when viewed within the context of the quilt, create visual noise, making a clear distinction between the data and visual cues. In contrast, to represent a lack of movement, we have chosen two 'straight' horizontal lines, which achieve the opposite effect, creating a visual, calming sense of 'still'. Because couples may be restless or still while touching, we chose to simply unite the two representational 'movement' lines into one to visualize periods of contact between the couple. Finally, we chose color to represent proximity, which is represented by 'heat scale' values--- blue, a cool color, indicates when the couple is far apart, orange, a little warmer, represents a closer proximity, and finally, red, a warm color, represents the closest proximity.<br><br>\nThe REMQuilt user interaction is quite simple. The first partner to get into bed interfaces with either a desktop or mobile application, simply inputting the time that the latest-sleeping partner wishes to wake up, as well as indicating that they are starting their evening of rest. REMQuilt tracks time, and when the wake-up time is reached, REMQuilt will trigger soft music and bring the projection to light to be viewed.<br><br>\nREMQuilt is comprised of a number of digital textiles: a Kinect camera for low light camera and depth tracking; libraries in openFrameworks to suggest movement and proximity; two conductive pillow cases and an Arduino to measure body contact between sleepers; a laptop computer and software to knit together all of these variables, and a projector mounted above the bed to illuminate the connections in the morning."},{"pitch":"Make everyday objects touchable and create interaction on them.","author":"Deqing Sun","title":"Touchable Objects","url":"http://itp.nyu.edu/shows/spring2012/touchable-objects/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1336671836_capacitive_mario_peach.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tTouchable objects implement capacitive sensing technology to our everyday objects. By hiding copper tapes inside fabric, we can make any fabric surface touch sensitive. We can touch our plush animal like a pet, and get feedback from it just like a real pet. We can simulate emotion and reaction in our toys to give them life. Also this technology can be implement on horizontal surfaces to turn ordinary surface to a trackpad. We can either play with our finger to recognize movement and gesture, or use some conductive model to play on a digital play board.<br><br><b>User Scenario</b><br>\n          \t\t\t\tFor a plush toy, people can touch it just like a pet. The movement of user's fingers will be analyzed and proper reaction such as a certain sound clip, vibration or movement to give feedback to user. And make people feel that they are having a conversation with a toy.<br>\nFor a trackpad, We can make a scalable track pad. It can be a small one to interact with our finger or large enough to interact with our feet or whole body. If we place conductive toy models on it, their position can be tracked and we can create a physical interactive user interface between player and computer."},{"pitch":"Plinko Poetry is a new interactive interface to design electronic poetic text.  Every player is a winner!","author":"Deqing Sun, Inessah Selditz","title":"Plinko Poetry","url":"http://inessah.com/portfolio/plinko-poetry/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1336063163_img_0092_700x640.png","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tPlinko Poetry is a new interface for electronic poetic expression. Drawing source text from current @nytimes and @FoxNews tweets, players can absurdly re-contextualize news headlines that are often overloaded with meaning. With roots in The Price Is Right and experimental blackout poetry- every player can be both a winner and a poet!<br><br><br><br><br>\nThe interface of Plinko Poetry uses Processing to display alternate scrolling lines of current tweets from the New York Times and Fox News. When a user drops a chip, it randomly hits pegs on the way down. The word under each peg that is hit is highlighted, with the untouched pegs automatically darkened. Plinko Poetry uses openFrameworks camera color tracking to determine which pegs have been encountered. When the chip comes to a stop, the user is left with a trail of blackout poetry which is then live tweeted to @PlinkoPoetry.<br><br><br><br><br>\nUltimately users will create a new corpus of ever changing poetic text based on the zeitgeist of current headlines.<br><br><br><b>Audience</b><br>\n          \t\t\t\tAnyone who likes weirdo poetry and has eyes?\t\t\t\t\t<br><br><b>User Scenario</b><br>\n          \t\t\t\tPlayer walks up to screen where text is scrolling. Player drops chip and text either highlights or blackouts depending on if the chip touched the corresponding peg. User is left with a trail of blackout poetry. Poems are then live tweeted to twitter and shown on a separate display.\t\t\t\t\t<br><br><b>Implementation</b><br>\n          \t\t\t\tIt's made of a LCD screen incased in layers of CNC'd wood and plexiglass.\t\t      \t\t<br><br><b>Conclusion</b><br>\n          \t\t\t\tA lot, hopefully it doesn't break!"},{"pitch":"An immersive, 3D, audiovisual simulation of time-travel into your own recent past","author":"Justin Lange","title":"The Time Travel Simulator","url":"http://itp.nyu.edu/~jl4554/blog/?p=367","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335768200_timemachine.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tThe Time Travel Simulator provides an interactive, personal experience of time travel to any one person at a time. The technology is simple: record 3D space using a Kinect sensor, track the head position, perspective and orientation of the current time-traveler, reconstruct a point cloud of how a particular point in time would look from the current physical perspective of the user and display that perspective in 3D using a head-mounted display (video goggles).<br><br><b>Audience</b><br>\n          \t\t\t\tAnyone! Particularly a small group of friends. The less technologically-minded, the better! And their friends. I suspect when folks go back in time, they'l want to walk over to where their friends were standing and listen to what they were saying a few minutes ago.\t\t\t\t\t<br><br><b>User Scenario</b><br>\"Hey, want to travel back in time? Yeah? Cool. OK, sit down and put on these goggles. Sure, I'll adjust the strap. More comfortable? Great. Are you seeing in 3D? Looks you're on the edge of a crazy wormhole? Yeah, that sounds about right. Cool, how about we send you back about five minutes? Good? OK, let me put in your earbuds. Can you hear me now? Nothing else? That's right, I'm speaking to you through my headset. In a moment, I'm going to send you back, and you can walk around a little. Move slowly, and stay within five or six feet of where you are now. We'll make sure nothing happens to you while you're engaged in the past, and everyone who's around you right now make sure not to bump into you while you're a-time-travelin'. Ready? Let's go!\"<br><br><b>Implementation</b><br>\n          \t\t\t\tA kinect, a chair, some mics, a few mac pros, a bunch of cables, a fairly quiet corner.\t\t      \t\t<br><br><b>Conclusion</b><br>\n          \t\t\t\tthree billion operations a second really isn't that much."},{"pitch":"A kinetic sculpture in which the movement of a pendulum clock powers the growth of a mechanical tree. This project bridges man-made and natural methods of time-keeping.","author":"Eric Hagan","title":"Time Immemorial","url":"http://itp.nyu.edu/~eah390/myblog/2012/time-immemorial/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335160423_timeimmemorial_320240.png","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tTime Immemorial is a kinetic sculpture inspired by Jean Robert-Houdin's Marvelous Orange Tree illusion and the automata of German clock makers. A mechanical tree sculpture slowly emerges and then recedes, powered by the periodic motion of a pendulum clock.  The synchronized movement forges a connection between natural and artificial methods of time keeping. Furthermore, in erasing the evidence of its own past, the cyclical movement of this project rethinks the reliance on sculptures and trees as memorial objects.<br>\nCreated using rapid prototyping techniques and digital fabrication equipment, Time Immemorial re-appropriates classical mechanisms to create life-like physical animation.<br><br><b>Audience</b><br>\n          \t\t\t\tThose who wish to connect with time in a physical sense.\t\t\t\t\t<br><br><b>User Scenario</b><br>\n          \t\t\t\tMy project is primarily about observation. The user input required is merely a simple winding to store potential energy. The tree feeds off of this mechanical energy over the course of time, withering as it reaches the end of the cycle.\t\t\t\t\t<br><br><b>Implementation</b><br>\n          \t\t\t\tThe sculpture is a combination of plexi glass, wood, delrin, steel rod and assorted steel rod.\t\t      \t\t<br><br><b>Conclusion</b><br>\n          \t\t\t\tIn building this project, I have broken a lot of acrylic both intentional and unintentionally. I learned that friction is either an ally or the strongest of enemies, to accept tolerances, and that nothing is ever right the first time."},{"pitch":"Found Letters is a historical novel-in-verse and interactive installation that translates history into fiction, immersing readers in a genealogist’s journey through the objects, documents, and letters she uses to reconstruct her family story.","author":"Carlin Wragg","title":"Found Letters","url":"http://foundletters.info","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335782307_fl_wragg_showimage.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tFound Letters begins on the print and digital page and finishes as an environmental installation: An abandoned nineteenth-century farmhouse, transported from western Kansas and installed in a New York City gallery. Pristine furniture situated amidst the physical decay becomes a sequence of waypoints along a journey through the Paul family’s memories. A scarred metal box, an opera glove, a cluster of windows activate on approach, hallucinatory images animate: A Paris dance hall, a Nazi flag flapping against white marble, burgundy wine in a carved glass, candle light, cobblestones. Wind whispers the wheat. As they explore, readers reconstruct the lives of the Paul family: Jack, Matilda, and Joseph, Oliver, Alice, and Laura, in a story that stretches from Savannah, Georgia, to London, to Nazi-occupied Paris, to a remote farm in western Kansas, exposing ambition and its aftermath, the mental wastes of war, and the persistent love of an American family.<br><br><b>Audience</b><br>\n          \t\t\t\tFound Letters is composed for poets and historians, artists and readers, especially those intrigued by new and novel interactions. Just as texts invite multiple readings, Found Letters offers readers different points of entry into the story. Soundscapes imbued with characters’ voices highlight the poem’s internal sonic rhythms. Found videos mapped to footage of decaying domestic spaces evoke characters’ memories.\t\t\t\t\t<br><br><b>User Scenario</b><br>\n          \t\t\t\tUsers see the interior of an abandoned Kansas farmhouse through three found farmhouse windows. When a user approaches, the scene changes to reveal figures moving around inside the house and they hear fragments of a sound poem.\t\t\t\t\t<br><br><b>Implementation</b><br>\n          \t\t\t\tThe installation is made of three found farmhouse windows lined with drafting vellum and suspended from the ceiling by sash cord. Rear projection-mapped videos animate the windows with images of three abandoned farmhouse rooms. A Kinect sensor detects the user's proximity to each window and triggers Max/MSP to change the video they see. A long fade reveals a new video that shows figures enacting scenes inside the house. These scenes have been filmed with actors against a green screen and keyed into the footage to give the feeling that the user has stumbled onto a house of ghosts."},{"pitch":"","author":"Anh Ly, Ji Hyun Lee","title":"Dinosaur Treasures","url":"http://itp.nyu.edu/~jhl589/myblog/portfolio/dinosaur-treasures/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1334850159_dino_cover2.png","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tThis project is about an interactive, hands-on, educational sandbox that allows kids to become real archaeologists. They can literally dig in the sand and look for virtual fossils. When the fossils are found, they animate and tell you the story of their life. It’s magic!<br><br><b>Background</b><br>We did a lot of research on dinosaurs: what era they lived in, where their fossils were found and who their ancestors were.  We also did a significant amount of work learning Pocode and the kinect library in c++.<br><br><b>Audience</b><br>\n          \t\t\t\tMostly kids, but anyone who likes dinosaurs and digging in the sand!\t\t\t\t\t<br><br><b>User Scenario</b><br>\n          \t\t\t\tPeople crowd around our sandbox.  They're instructed to dig for dinosaur fossils.  When someone finds one, the dinosaur comes to life and tells you their story.\t\t\t\t\t<br><br><b>Implementation</b><br>\n          \t\t\t\tThe sandbox was constructed with wood, and it's filled with sand. There's a projector and a Kinect mounted overhead.  We've already used this setup during User Testing Day, and it worked out quite well.\t\t      \t\t<br><br><b>Conclusion</b><br>\n          \t\t\t\tWe discovered that sand makes a terrific surface for displaying images. We learned that people have a lot of fun digging in our sandbox and that this could be a great way for kids to learn in a hands-on exploratory way."},{"pitch":"Risky Listy turns any email list into a fantasy sports game.","author":"Hsiao-Wen Chou, Naliaka Wakhisi, Sean McIntyre, Sheiva Rezvani, Zach Schwartz","title":"Risky Listy","url":"http://riskylisty.boxysean.com","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1336602125_logo.gif","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tRisky Listy is a fantasy sport played on existing email lists and the players are the emailers on the email list.<br><br>\nWe’ve taken the liberty here to show you the similarities and differences between fantasy sports and Risky Listy.<br><br>\nFantasy Sports: Manage a team of highly paid athletes<br>\nRisky Listy: Manage a team of underpaid office lackeys<br><br>\nFS: Obsess over stats sheets<br>\nRL: Obsess over the nuances of your coworkers<br><br>\nFS: Injuries are disastrous for your team<br>\nRL: Holidays are disastrous for your team<br><br>\nFS: Athletes track their own fantasy stats<br>\nRL: Emailers have no idea what’s going on<br><br>\nFS: Pay Tonya Harding to help you win<br>\nRL: Coerce your coworkers to help you win<br><br>\nFS: Played during the sports season<br>\nRL: Every workday is gameday<br><br>\nFS: Athletes earn you points based on how they perform<br>\nRL: Emailers earn you points based on how they email<br><br>\nTo play Risky Listy, you need to download the Risky Listy software and use the game setup wizard. You will first pick the email list you want to play on and who the team managers will be. Besides the managers, no one else on the email list needs to know you are playing Risky Listy, and Risky Listy will not publish that you are playing Risky Listy.<br><br>\nNext you will choose choose the game rules, Risky Listy scoring categories, and details like when the game starts and ends. Risky Listy features standard rules based on the traditional fantasy sports “Rotisserie” and “Head to Head” models, but Risky Listy allows you to customize the rules so that you have the most fun.<br><br>\nWhat are the Risky Listy scoring categories? Well, fantasy baseball has HRs, RBIs, wins, and ERA. Fantasy football has TDs, rushing yards, sacks, and turnovers. Risky Listy has attached images, replies received, one-word emails, and late night emails, and many more!<br><br>\nYour Risky Listy experience will begin with a live draft. Risky Listy will generate pre-season rankings of the emailers so there is a general understanding of how well the emailers perform in the Risky Listy scoring categories. You and the other Risky Listy managers will take turns selecting list emailers for your teams. One restriction for the game is team managers are not able to pick themselves.<br><br>\nYour game commences following the draft. Each day you will have the option to change the lineup of emailers on your team who will earn you points for the scoring categories, propelling you to the top!<br><br>\nPlay at the office! Play at home! Play at school! Recommended number of players: 3+.<br><br><b>Background</b><br>Fantasy sports were invented in 1980 at a New York City restaurant, where friends would meet and play fantasy baseball. By 1988 there were an estimated 500,000 people playing across all fantasy sports, including football, soccer, and baseball. With the personal computer revolution and the rise of the Internet in the mid-90s, fantasy sport moved to online platforms and now is a multi-billion dollar industry.<br><br>\nRisky Listy is not the first esoteric game based on fantasy sports. Notably, Fantasy Congress was launched in 2006 to let players pick congressmen and congresswomen to earn points based on how they voted on bills. There has been at least one prior attempt at ITP to create word-based fantasy-style games operating on the ITP email list. Risky Listy, however, attempts to adapt the tried-and-tested fantasy sports model to email lists.<br><br>\nIn Risky Listy, the subjects of the game are not untouchable athletes, celebrities, or public figures. Instead, the game lens is turned to unsuspecting peers or co-workers who are the subjects of attention by the few team managers. The game raises questions of surveillance, invites subtle manipulation in our online chatter, and challenges permissive structures.<br><br><b>Audience</b><br>\n          \t\t\t\tOffice workers, open source developers, anybody on a mailing list\t\t\t\t\t<br><br><b>User Scenario</b><br>\n          \t\t\t\tHere we will describe how we intend to show the game in the setting of the Spring Show where we are given 30 seconds or less of someone’s attention.<br><br>\nWe, the creators of Risky Listy, will play a game of Risky Listy over the course of the two days of the Spring Show using a hypothetical email list of archetypal emailers.<br><br>\nThe archetypal emailers will include “the tough boss”, “the passive-aggressive employee”, “the mad IT guy”, “the apologetic intern”, and “the disconnected CEO”.<br><br>\nWe will invite guests at the Spring Show to send emails to our hypothetical email list as one of the archetypes, earning us, the players of Risky Listy, points.<br><br>\nWe request one large screen to show the real-time Risky Listy scoreboard, and one laptop or desktop computer to provide an interface for guests to write emails as the archetypes.\t\t\t\t\t<br><br><b>Implementation</b><br>\n          \t\t\t\tRisky Listy was made using Python and Django and is an open source project. We aim to release a 1.0 version that is easily installable and usable on a variety of platforms for widespread usage.\t\t      \t\t<br><br><b>Conclusion</b><br>\n          \t\t\t\tWith the first release of Risky Listy, we discovered that the ITP email list is rather unique. It is a flat hierarchy that is atypical of other lists. When the mechanics of Risky Listy were discovered by a pair of emailers who were not team managers, they unabashedly spammed the list skewing the results of the game in favor of the managers who owned those emailers.<br><br>\nIn reality, work email lists are hierarchical and higher-ups would possibly frown upon workers who play Risky Listy, let alone punish unabashed Risky Listy spam. By designing the game to be played covertly, workers will not ask for permission to play Risky Listy and likely will not tell co-workers outside the game that the game is being played. They will instead rely on being sneaky, and if adventurous, subtle manipulation."},{"pitch":"Swings is a site-specific interactive piece that creates an immersive auditory experience, where multiple sounds, noise and narratives are layered via swinging patterns of the user.","author":"Claire Mitchell, Engin Ayaz, Patrick Muth","title":"swings","url":"http://dl.dropbox.com/u/7870459/ITP%20Projects/SpatialMedia/Project2_swing_mar28.pdf","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335732985_swings_itpshow_apr29.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tSwings are at once exhilarating and calming. By proposing to integrate interactive swings among the existing architectural structures of the urban environment, we allow people to remain immersed in the context of the city, while encouraging reflection, and appreciation of the layered history. In the ITP show context, we will demonstrate the experience of an interactive swing, and invite the user to imagine the manifestations of this setup in different parts of New York City. Technically, the project uses the iPhone to pull in acceleration data, uses openFrameworks to modify and layer the samples, and bluetooth speakers for sound output. To emphasize environmental stewardship, the design of the swings incorporates salvaged wood and organic manila rope.<br><br><b>Background</b><br>We researched possible sites for installation, audio recording options, iPhone application development process (Objective C and openFrameworks), and swing construction methods. After weeks of iteration, we have settled on a former mannequin factory that has been converted into an A.I.R. building in Williamsburg as a speculative site and decided to use a combination of sewing machines, children at playground and ambient sounds. For the ITP show, we will refer to the site-specific nature of the work by projecting images from that site across the user.<br><br><b>Audience</b><br>\n          \t\t\t\tOur audience is any adult who is longing for the swinging experience (hard to come by in nyc), or any child that would be interested in swinging outside the playground context. We are also interested in turning this into a city-wide project, with multiple installation sites, so anyone who is well-connected to public space areas in the city is our targeted audience for next steps.\t\t\t\t\t<br><br><b>User Scenario</b><br>\n          \t\t\t\tThe user is intrigued by the swing. A small piece of writing indicates \"swings is an interactive installation. please download the \"swings\" app from the itunes store, turn on your bluetooth, and insert your phone below.\" The user quickly downloads the free app, and inserts his/her phone underneath the swing. The soft padding in the dedicated slot underneath the swing prevents the phone from being scratched or falling during the swinging action. As the user starts swinging, multiple sounds are subtly layered. These sounds emanate from the bluetooth-connected speakers hanging above. The user slowly realizes the dynamics of the system: - The longer someone keeps swinging, the more sounds are introduced to the system. - There is a \"sweet spot\" of swinging cadence that reveals each sound layer in its full clarity. The system does not encourage the \"microphone phenomenon\", i.e. shout into the microphone or swing as high as possible. Instead, the system suggests a repetitive, mid-force swinging experience for reflection. <br><br><br>\n- The fade-in and fade-out effects are used to transition between different states: the start, the middle, and the end.\t\t\t\t\t<br><br><b>Implementation</b><br>\n          \t\t\t\tThe swing is constructed out of salvaged wood from an old Singer sewing machine crate and organic manila rope, with small pieces of black foam used as protective material for the iPhone. The bluetooth speakers are used as output, and will be securely hung at the ceiling.\t\t      \t\t<br><br><b>Conclusion</b><br>\n          \t\t\t\tWe realized that openFrameworks' iPhone library is very user-friendly and appropriate for projects such as swings, and Objective C is a bit more complex and seemingly cryptic.<br><br><br>\nWe learned that testing the project with the swing, rather than a hypothetical swinging motion, helped us make progress much faster in terms of mapping the sounds.  We were afraid to have iPhones or swing prototypes broken in the process, but nothing happened at the end of the day."},{"pitch":"The incredible works done by the digital fabrication class of Spring 2012","author":"Trent Rohner","title":"Digital Fabrication Class","url":"http://itp.nyu.edu/shows/spring2012/digital-fabrication-class/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1336681703_photon37n.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tFrom ceiling tiles to benches to jointed plywood cats, the 2012 Spring digital fabrication class has created some amazing work.  Using techniques such as the CNC, laser cutter, 3D printing, along with a number of different manufacturing techniques, drawing techniques (Rhino, VectorWorks, Illustrator to name a few) and good old fashion ingenuity."},{"pitch":"“Choose one” is an interactive installation that tells stories by responding to audience choices in unexpected ways.","author":"Chien-Yu Lin","title":"Choose One","url":"http://itp.nyu.edu/shows/spring2012/choose-one/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335236570_choose_one.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\t“Choose One” is an interactive story installation that requires participating audience to make decisions about how to interact and to consider the impact of their decisions on the resulting story. By moving a weight to tip a balance board in one direction or another, participants cause light-projected objects on the board, such as a seed or an insect, to move either toward a water-based story world or toward a dirt-based story world. Depending on the previous interactions and the type of object moved, the result, in the form of an animated story ending, is different from participants’ original expectations, as well as  from what they have learned from previous iterations, making the experience continually surprising.<br><br>\nThe intent of this experimental piece is to explore the idea that by providing unpredictable reactions to participants’inputs, they will be motivated to interact more, which might  create an interesting narrative in itself.<br><br><b>Audience</b><br>\n          \t\t\t\tAdult, People who love stories and art.\t\t\t\t\t<br><br><b>User Scenario</b><br>\n          \t\t\t\tBy moving a weight to tip a balance board in one direction or another, participants cause light-projected objects on the board, such as a seed or an insect, to move either toward a water-based story world or toward a dirt-based story world. Depending on the previous interactions and the type of object moved, the result, in the form of an animated story ending, is different from participants’ original expectations, as well as  from what they have learned from previous iterations, making the experience continually surprising.<br><br><br><b>Implementation</b><br>\n          \t\t\t\tHand-drawing animations,  Projection,  Openframework, Arduino"},{"pitch":"OpenBooks is an interactive table that allows you to flip through a book's pages while easily reading other people's reviews on it.","author":"Hannah Davis, Luis Daniel Palacios Morton","title":"OpenBooks","url":"http://www.luisdaniel.com/blog/portfolio/spatial-media-interactive-book-review-table/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335795780_table2.png","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tOpenBooks is an interactive table specifically designed to integrate the best part of online book shopping into the experience of browsing your local bookshop. As more people become accustomed to reading other shoppers’ reviews before making their purchases, OpenBooks provides the same opportunity in-store.<br><br><br>\nAn OpenBooks table could be set up in any location around the store, or even in the bookstore’s cafe area. The interface is simple. As a user places a book on the table, the barcode/ISBN number is scanned and relevant reviews are read from the internet — in this case, Amazon.com. Multiple books can be placed on the table so that the user can compare and contrast reviews in order to choose the best book. The user is able to scroll through the reviews and also see editorial opinions as well.<br><br><br>\nOpenBooks uses the Kinect and the OpenCV libraries to track the books. It is programmed using OpenFrameworks. The book’s barcodes are scanned by a webcam and decoded with Processing."},{"pitch":"This project takes footage from a trans-american train trip and transforms the ITP hallway into a majestic viewing car.","author":"Rose Schlossberg","title":"usa west to east","url":"http://itp.nyu.edu/~rs3836/blog/2012/04/10/hallway-installation-2/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335795924_p1000756.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tThis project seeks to spread back out in space the distance absorbed by the traveller when crossing America by train. Video clips from different parts of the trip are looped on 7 screens along the hallway to create the sensation of motion and depth towards the horizon, out the \"window\" of these screens.<br><br><b>User Scenario</b><br>\n          \t\t\t\tWalking down the hallway, you are transported.\t\t\t\t\t<br><br><b>Implementation</b><br>\n          \t\t\t\tVideo files"},{"pitch":"A video sculpture installation based on the body of work of Richard Serra, focusing on his process and especially on his verb list to execute actions on video itself - or rather, on light, the essence of video.","author":"Jason Rosen, Karolina Ziulkoski","title":"To Hommage","url":"http://itp.nyu.edu/~krz212/nosleeptillbrooklyn/2012/04/to-hommage/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335796176_hommage_image_show.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tFor this project the brief was to develop a video sculpture installation based on an existing sculpture. We decided to base our installation on the body of work of Richard Serra, more precisely in his process for developing sculptures. Being a minimalist, we decided to execute the actions he performs on materials on the essence of video, light, creating a tangible material out of something you can't see.<br><br><b>User Scenario</b><br>\n          \t\t\t\tBeing it an art installation, people should experience it by themselves, with no guidance. The explanation will be available as text, but it is not necessary to know it before experiencing it."},{"pitch":"A kinetic sculpture of small wooden houses and a windmill, when you blow on the windmill all of the houses spin as if in a tornado.","author":"Ben Light","title":"Tornado Alley","url":"http://itp.nyu.edu/shows/spring2012/tornado-alley/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335796836_7119504345_76d26dff84_z.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tLittle wooden houses are placed in a row with an old fashioned windmill in front of them.  When the user blows on the windmill the houses begin to spin.  All of the houses spin at different speeds and one needs to blow pretty hard to get the last house in the line to spin.<br><br><br><br>\nThis interaction is created using a series of DC motors for each house and an arduino reading the signal from the spinning fan to control the speeds of the motors.<br><br><br><br>\nWarning: Using this sculpture can cause sever light headed-ness.<br><br><b>Background</b><br>I have been working with 3D printed windmills in another class (sculpting data) and have grown to love the shape.  I like the idea of getting the piece to move by blowing on it.<br><br><b>Audience</b><br>\n          \t\t\t\tPeople of all ages.  I think people will blow on it till they get very light headed.\t\t\t\t\t<br><br><b>User Scenario</b><br>\n          \t\t\t\tA user approaches the piece and sees 5 non-spinning wooden houses.  The user blows on the windmill and one by one the houses start to spin.  The windmill needs to spin very fast to get the last house to spin.  I see many people giving up before they get all of the houses to spin (or pass out from hyper ventilating).\t\t\t\t\t<br><br><b>Implementation</b><br>\n          \t\t\t\tAlmost all of the materials came from the junk shelf or previous projects - nothing bought specifically for this piece.\t\t      \t\t<br><br><b>Conclusion</b><br>\n          \t\t\t\tI've burned out a few motors and gone through many iterations on my circuit.  But I am much more confident with building circuits and programming Arduino.  To quote Todd, \"this stuff is easy\"."},{"pitch":"An interactive children's play space that takes them inside of the Heart, Veins, and Capilliaries.","author":"Robin Reid","title":"Circulatory Playground","url":"http://www.itp.nyu.edu/~rnr217/myblog","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335800153_heartthumnail.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tAs a part of our Design for Playful learning course, we designed a playground exhibition that would help young students grasp a few core concepts about the Human Cardiovascular Circulatory System.  Our playground exhibit is a mix of sound installation, video projection, physical computing, and hands on activities that asks students to become like  blood cell and travel through the body to understand what happens to a drop of blood as it leaves the heart.  Our goal is to create an informal learning experience to support STEM initiatives to get children excited about learning science.<br><br><b>Background</b><br>Our background material came primarily from the course readings. Our biggest inspiration was from a study done by Mary Arnaudin and Joel Mintzes about common misconceptions that early science students have about the human cardiovascular circulatory system.<br><br><b>Audience</b><br>\n          \t\t\t\tYoung science students ages 8 - 14.\t\t\t\t\t<br><br><b>User Scenario</b><br>\n          \t\t\t\tUsers see a small step up to a stand and activate an audio recording that gives them the mission to take on the role of a blood cell to deliver oxygen and nutrients throughout the body and to collect carbon dioxide.  They then go into the heart(a small tent), where they discern the sounds of the left and right ventricle and learn what they are expected to do. From an air tube, they collect particles of oxygen then crawl through the tunnels that represent the arteries and the veins depositing oxygen and picking up waste. The tunnels have animations projected that represent if they are carrying oxygenated or de-oxygenated blood-they also have sound installations that indicate if the blood flow is stronger or weaker.\t\t\t\t\t<br><br><b>Implementation</b><br>\n          \t\t\t\t2 tents (or 1 tent partitioned off inside) 2 tunnels, 4 projectors, a few force sensors, an oxygen station. We can adjust the pieces to fit the space that we get.\t\t      \t\t<br><br><b>Conclusion</b><br>\n          \t\t\t\tWe wanted to know if it is actually possible to use play to teach abstract concepts. Does the physical circuit become a distraction? We learned that it is possible, if not super successful for them to be physical and to learn/discover/reinforce science knowledge."},{"pitch":"Group calling and collaborating made easy","author":"Philip Groman, Robbie Tilton","title":"Rehuddle","url":"http://www.rehuddle.com","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335629810_rehuddle_springshow.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tRehuddle is the web's simplest and most convenient way to set up and manage conference calls. Huddles are created from a simple web form that generates a unique online meeting room with automatic sharing features. As callers connect to the huddle, the site displays active participants and offers sharing and collaboration features.<br><br><b>Background</b><br>While developing the project we have looked at a variety of other conferencing solutions and social applications, including online music, instant messaging, web radio, and online education.<br><br><b>Audience</b><br>\n          \t\t\t\tOur target audience is small creative businesses that cannot afford expensive phone systems and are looking for a free, easy and fun solution. Our secondary target market is for anyone looking to speak and share information with two or more people.\t\t\t\t\t<br><br><b>User Scenario</b><br>\n          \t\t\t\t1) A group of professionals are collaborating on a project and need to instantly set up a phone based conference call.  2) One participant uses Rehuddle to create a conference room. 3) They share the link with colleagues. 4) Everyone calls in except for one person who is not online. 5) The group auto generates a call to bring the missing person onto the line.\t\t\t\t\t<br><br><b>Implementation</b><br>\n          \t\t\t\tThe phone system is built on Asterisk 10 running on our own Rackspace server. The code is written in Ruby/ Sinatra, Node.js, Jquery/ JavaScript. Calls are routed through Flowroute and SMS functionality uses the Twilio API.\t\t      \t\t<br><br><b>Conclusion</b><br>\n          \t\t\t\tWe have developed a much deeper understanding of telephony and conferencing calling systems, dynamic web development and server side scripting and systems administration. By presenting ourselves with an interesting design challenge to create a new interface for setting up conference calls, we undertook a rigorous design process and experimented with a series of different user interfaces and experiences."},{"pitch":"Cops, full of piss and vinegar, share their experience of life \"on the job.\"","author":"Heather Velez","title":"Piss and Vinegar","url":"http://velezheather.com/cop-stories","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1336590685_heather7.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tA collection of audio stories from retired cops reflecting on their lives \"on the job.\" Piss and Vinegar stitches together a narrative of what it's like to be a cop. The stories are accompanied by \"artifacts\" that the subjects provided that showcase some of those memories.<br><br><b>User Scenario</b><br>\n          \t\t\t\tUsers will approach display case and view objects. They'll be able to listen to the stories associated with the objects through the screen interface. Each cop will have his own page that includes a photo, short bio, any other associated objects (e.g. newspaper clippings, photos), and all of the audio clips. There will be a primary audio clip that's tied to the object displayed and several other clips with additional stories."},{"pitch":"Cave Temple for Training Digital Ninjas is an interactive art piece that project holograms digitally echoing events around it.","author":"Owen Roberts, Sae-Wook Huh","title":"Cave Temple for Training Digital Ninjas","url":"http://itp.nyu.edu/~obr208/blog/2012/05/09/comp-cam-final-documentation-1/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1336696397_imag0442.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tCave Temple uses multiple digital cameras that dissect the visual informations that they take in and re-assembles the visual elements in different layers and time to re-invent new and spiritual meanings for the events that surrounds our daily lives.<br><br><b>Background</b><br>Our final project, currently titled “Cave temple for training digital ninjas,” incorporated ideas and work from a few of the projects we worked on during the semester.  We were interested in using Pepper’s Ghost along with live video feeds to put people in a new space.  At first we were interested in doing an installation in an abandoned store front, using a video feed with some effects like the video filters I had made and background cancelation so people could see themselves as ghosts in real time.  Unfortunately, none of the real estate agents I tried calling ever responded to my messages.<br>"},{"pitch":"Enhanced Visualization (EV) Therapy is a technologically mediated mind-body therapy founded on traditional healing practices and immersive interactive technologies.","author":"Jason Stephens","title":"Enhanced Visualization (EV) Therapy","url":"http://itp.nyu.edu/shows/spring2012/enhanced-visualization-ev-therapy/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335242942_thesispict_projectdbfrompsd.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tEV Therapy is an integration of therapeutic massage, augmented reality, interactive video projections, and realtime visual feedback from an out-of-body perspective.  Treatments involve the use of video goggles, which provide both client and therapist with a live view of the healing session from a single overhead perspective.  Throughout the session, both client and therapist see what the other sees, a realtime view of themselves in a healing space from a shared point-of-view located outside both of their bodily borders.  Clients watch themselves as the therapist creates, projects, and guides interactive visualizations and augmented reality objects onto the client’s body.  A gesture based and wireless touchscreen interface untethers the therapist from a separate control mechanism, allowing the therapist to remain available to administer therapeutic bodywork.  The end result is a multi-sensory multi-media healing experience.<br><br><b>Background</b><br>Here's something on Cybernetic Acupressure:  http://itp.nyu.edu/~js5346/jayblog/<br>\nI'm calling it AcuNetics, and it represents just a small portion of what i'm working on.<br><br><b>Audience</b><br>\n          \t\t\t\tEV Therapy is geared toward anyone interested in receiving a session.\t\t\t\t\t<br><br><b>User Scenario</b><br>\n          \t\t\t\tUsers are invited to a 15 minute session inside the healing space.  The first few minutes involve an quick introduction, an assessment, and a brief demonstration of how to wear the video goggles.  The users/clients are then treat to a short session of Enhanced Visualization Therapy\t\t\t\t\t<br><br><b>Implementation</b><br>\n          \t\t\t\tThe space is already setup at ITP.  In order to be functional for the show, it need only remain in its current location.\t\t      \t\t<br><br><b>Conclusion</b><br>\n          \t\t\t\tLearning to program is like peeling an onion..."},{"pitch":"Watch the \"Van Gogh Palette\" paint your portrait in the impressionist style in front of your eyes","author":"Oya Kosebay, zaquerie applepress","title":"Van Gogh Palette","url":"http://itp.nyu.edu/~ok467/blog/?p=517","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335797564_vangoghpalette.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tVan Gogh Palette is inspired by the brush strokes of the greatest impressionists. It is using Van Gogh's brush strokes to paint the portrait of the person who is sitting in front of it's easel. The processing sketch is slowly start to paint your portrait as if an original painting would happen. You can sit as long as you like to see more detail work until the next person triggers the palette to start the newly captured portrait. The project is trying to take the user to an old time favorite process of fine art, portraits by great impressionists.<br><br><b>Background</b><br>Van Gogh used his brush strokes in a very particular way. He created a pattern that is recognizable. The processing sketch is using a pointillist style to mimic those strokes. The reason that I wanted to create a portrait of the user is related to the \"Self Portrait\" understanding in fine art. I think it is a way to communicate for the painter to show their appearance to the audience. There is something there about the need to show who they are rather than just only being known by the work they have done.<br><br><b>Audience</b><br>\n          \t\t\t\tAnyone, from 3 year old to the elderly. I think anyone can find something about themselves in this project. It is great to be able to communicate the impressionist to the younger generation also.\t\t\t\t\t<br><br><b>User Scenario</b><br>\n          \t\t\t\tThe user comes in and sits down in front of an easel that has a canvas. As soon as he/she sits down the palette starts to paint his/her portrait, stroke by stroke. This process doesn't take minutes but it takes a while allowing the user to watch their portrait to be revealed. The palette continues to put brush strokes of the details until the next person comes in and sits on the chair.\t\t\t\t\t<br><br><b>Implementation</b><br>\n          \t\t\t\tIt is made of a processing sketch that is mimicking Van Gogh's brush strokes. The projection is mapped to fit perfectly to the canvas so it looks convincing. The trigger MaxMsp patch is looking for a proximity sensor number to change for the palette to start painting. As soon as the user sits on the chair the reading from the proximity sensor triggers the web cam hidden on the easel. The camera captures the pose and instantly the program starts to paint the new painting.\t\t      \t\t<br><br><b>Conclusion</b><br>\n          \t\t\t\tI realized that I can recreate greatest painter's works using processing and the power of video mapping to create augmented realities that is mesmerizing."},{"pitch":"A multimedia installation and performance piece exploring the confluence of the sacred and profane, object significance to memory, and grieving in historic and contemporary meaningful ways.","author":"Bona Kim, Hanna Kang-Brown","title":"Jesa","url":"http://itp.nyu.edu/shows/spring2012/jesa/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1336746349_untitled-1.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tJesa is a Korean cultural ritual commonly practiced to remember ancestors and the recently dead. Jesa is usually held on the anniversary of the ancestor’s death and is an intimate family ritual that is held in a private home with no public participation.  Though it is a centuries-old practice, jesa continues to be practiced and modified to meet modern cultures and practices.<br><br><br><br><br><br><br><br><br><br>\nJesa involves a ritual of food, wine, and candles laid out on the table, symbolic gestures such as bowing, and a decorative folding screen (that often has landscape paintings or Buddhist writings). One of the traditional ideas is that the spirit of the dead comes to eat the food that the family lays out.The content of this piece will involve video and still images that are engaged and brought to life through the movements of the par- tipant and interactions with objects laid on the table.<br><br><br><br><br><br><br><br><br><br>\nA multimedia installation and performance piece meant to be performed within the context of a home.  Using RFID sensors, video and image processing, the piece seeks to explore the confluence of the sacred and profane, object significance to memory, and grieving in historic and contemporary meaningful ways within a multi-cultural, multi-religious context.<br><br><b>Background</b><br>The content is informed by my paternal grandfather's death.  After surviving the Korean War, he ended up managing the commissary on American military bases in Vietnam and then immigrating to Los Angeles, California where he started out as a contract worker for Los Angeles City Hall, and then went on to own a string of liquor stores and grocery stores.  At one of those stores, he was shot several times mistakenly by undercover LAPD and suffered major health ailments throughout his life.  The video content is of his former places of residence and work.  <br><br><br><br><br><br><br><br>\nResearch on the jesa ceremony and history was also done to inform our decisions.  Jesa is a private home ceremony that is practiced both traditionally and superstitiously as well as in modified, contemporary ways to fit the modern family.<br><br><b>Audience</b><br>\n          \t\t\t\tAnyone interested in ritual, culture, multi-religious, multi-cultural explorations.\t\t\t\t\t<br><br><b>User Scenario</b><br>\n          \t\t\t\tWe will have a video of myself performing a scenario of the user experience playing in front of the installation, such as a wall just outside the installation space.  This will give users a sense for what could occur in the space and set the tone.  Users are then invited to move tagged objects from a small table to a bigger table set up with ceremonial vessels in which they can place the object and see a corresponding video projected on to the folding screen.\t\t\t\t\t<br><br><b>Implementation</b><br>\n          \t\t\t\tFolding Screen (foam board, muslin, hinges)<br><br><br><br>\nfloor mat<br><br><br><br>\n2 Korean traditional tables<br><br><br><br>\nClay/Acrylic ceremonial vessels and candlesticks<br><br><br><br>\nRFID readers and tags<br><br><br><br>\nProcessing<br><br><br><br><br><br><br><br>\nWe hope to develop sound during Thesis week for this project.<br><br><br><br><br><br><br><br>\nThe ideal space for this project is a small room, such as a faculty office.  The privacy and intimacy that that kind of space could afford is perfect for this piece to carry its weight.\t\t      \t\t<br><br><b>Conclusion</b><br>\n          \t\t\t\tI learned a lot about creating a concept from scratch, developing it over a 14 week period, enlisting help and finding a partner who was as passionate about it as I.  I also learned to work with RFID readers, tags, and movies in Processing, as well as learning to follow my instincts, flying to Los Angeles to get footage, and working with limited resources on the floor.  I learned to think through user experience and see the value of performance when a piece can't possibly just be an intuitive user experience."},{"pitch":"A space where three people's creativity meets constraints and produces... art?","author":"Adekunle Somade, Roopa Vasudevan, Ryan Viglizzo","title":"Gallery 182","url":"http://gallery182.tumblr.com/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335798397_gallery182.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tA creative and playful task that sparks conversations between people who typically dont work together, Gallery 182 is a diorama of a traditional gallery space. The shared experience for the group invites future interaction and offers the community a weekly insight into the creativity of their fellow student.<br><br><b>Background</b><br>We looked at how the first semester Applications class forged lasting relationships between group members. We looked at the dynamic of hackathons, where the process of creating as a team is sometimes more important than the output. We looked at existing ITP student-run traditions that have became fixtures due to the efforts of their founders and the way that they are passed on to the next generation.<br><br><b>Audience</b><br>\n          \t\t\t\tOur target audience for Gallery 182 long-term is the ITP community. Our audience for the lockers that are created at the show are the visitors.\t\t\t\t\t<br><br><b>User Scenario</b><br>\n          \t\t\t\tWe show visitors what has already been created in Gallery 182. We invite 3 visitors that are interested in the concept to collaborate in the design challenge of building an installation around a theme in a locker using only the materials that we give them.\t\t\t\t\t<br><br><b>Implementation</b><br>\n          \t\t\t\tA locker with a small lighting rig. A box of materials. A theme. A sense of humor.\t\t      \t\t<br><br><b>Conclusion</b><br>\n          \t\t\t\tWe learnt about group collaboration and miniature creativity."},{"pitch":"modifying the occupied landscape through the use of a series of fans and switches","author":"Christopher Egervary","title":"ripple landscape","url":"http://itp.nyu.edu/shows/spring2012/ripple-landscape/","image":"images/none.png","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tby modifying the occupied landscape, new experiences and uses are discovered. through the activation of switches, a seires of fans can be turned off and on creating a varied and constantly changing landscape for occupation.<br><br><b>User Scenario</b><br>\n          \t\t\t\tthe user approaches a quiet serene surface with a set of switches on one side. as soon as one of the switches is flipped, fans under the surface turn on, altering what was previously a typical flat plane. as the further switches are flipped, varying patterns of fans turn off and on creating a constantly changing surface."},{"pitch":"Branch Out: Start A Conversation With A Stranger is an interactive installation that is designed to allow people in public spaces to speak to each other at a distance.","author":"Lynn Burke, Stefanie Kleinman, Tak Cheung","title":"Branch Out: Start A Conversation With A Stranger","url":"http://itp.nyu.edu/shows/spring2012/branch-out-start-a-conversation-with-a-stranger/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1336593569_branchout.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tWe have designed and constructed two audio devices that look like tree stumps. Each stump contains \"walky-talky\" components and speakers and people are able to talk into one stump and be heard through the speaker of the opposite stump. The concept is that these stumps can be placed in public spaces to allow people to speak through them and communicate with new people.<br><br><b>Background</b><br>In preparation for this project, we researched many stranger interactions, social behaviors, and public art installations. We aimed to design an installation that was fun and simple to use that would easily translate the message of the importance of communicating with others.<br><br><b>Audience</b><br>\n          \t\t\t\tOur target audience is broad and includes anybody who would like to test/use the stumps and talk to other people.\t\t\t\t\t<br><br><b>User Scenario</b><br>\n          \t\t\t\tEach of the stumps is placed on either end of a public space. People passing by one stump can speak into the microphone built into the top of the stump and someone passing by the opposite stump can respond. The response is heard through the speaker at the top of each stump. <br><br><br><br>\nStrangers can use the stumps as a communication device/tool to talk about anything they like and even try to locate one another. <br><br><br><br><br><b>Implementation</b><br>\n          \t\t\t\tThe stumps are made of cardboard tubes which we painted. The electronic components include walky talkys, speakers, microphones, and batteries. We wanted the pieces to be as attractive and simple as possible while having a clear function and purpose.\t\t      \t\t<br><br><b>Conclusion</b><br>\n          \t\t\t\tWe learned a lot from this project. We went through several iterations of the look and feel of the design and also struggled with electronic difficulties when trying to avoid interference with the walky talkies. We needed to balance out the power from the batteries and make sure that the quality of the sound was good so we learned from experimenting with those factors. <br><br><br><br>\nWe also learned about social behavior and interaction patterns."},{"pitch":"Negative Space is a convergence of old and new media, an exploration of paint as an interface. It allows children and families to experience how the painting they create is augmented with projected animations.","author":"Adria Navarro-Lopez, Hanna Kang-Brown","title":"Negative Space","url":"http://itp.nyu.edu/shows/spring2012/negative-space/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335801423_proposalimageshow.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tWhen visitors arrive, a glowing frame seems to move like a fluid, or a cloud, inviting them to start painting into the black canvas. They draw their first strokes and an animation is projected on them, transforming the paint into ponds and waterways. The animation affects their following decisions, establishing a conversation between the artwork and its authors.<br><br>\nOriginally design for Children's Museum of the Arts, Negatie Space uses curiosity and discovery to encourage the creation of a collaborative piece that the family can take away as a memento."},{"pitch":"Gortha is an interactive table that teaches children about digestion and nutrition through comparative biology","author":"Eszter Ozsvald, Greg Dorsainville, Roopa Vasudevan","title":"Gortha","url":"http://itp.nyu.edu/shows/spring2012/gortha/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1336683284_img_9960.jpg","more":"<!--<div class=\"entry\">-->"},{"pitch":"I Wish I Said Hello is a street art project that attempts to take the incomplete digital network of 'Missed Connections' – classifieds from Craigslist – back to the public space.  \nThis way we address the irony of our permanent connection through social media, as well as the poetics of everyday life.","author":"Adria Navarro-Lopez, Lisa Park","title":"I Wish I Said Hello","url":"http://iwishisaidhello.org/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335802312_show_thumb.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tThere is serendipity about 'Missed Connections'. Although there is very little chance of success, its existence creates hope in people that they will meet love of their life. <br><br><br><br><br><br><br><br>\nOur project tries to encapsulate specific Missed Connections stories into stickers. We combine parts of the original text with graphical elements that resonate with it. We use a common, universal style derived from public signage, as well as shapes and colors that imply the digital origin of the story. Once the images are created, they are placed at the exact location where that missed connection happened, and documented on the website.<br><br><br><br><br><br>\nCurrently, six pieces are installed in the areas of East Village, Soho, Metro Station, and Times Square, but our goal is to provide a method that's simple enough for collaborators to join and spread the movement through other cities."},{"pitch":"A work of art that explores the decay of digital data and brings awareness to the physical nature of digital content.","author":"Johann Diedrick, Robbie Tilton","title":"Digital Decay","url":"http://robbietilton.com/digital-decay","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335802549_digital_decay.png","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tImplied in the act of digital mediation is the ability transmit a message, with faithful reproducibility and no degradation of quality, to someone else in the future. We want to examine the nature of digital information as still retaining material qualities — it can wear, corrupt, disintegrate and decay much like any other physical material. If we decide to communicate through digital media, we risk the possibility of losing the content of the message and its meaning over time. The hallway becomes a metaphorical space to represent the linearity of time as well as the direction of one-toone communication. By slowing down the process of digital mediation and representing its eventual decay through audio and visuals, we intend to make visitors rethink ideas of how digital media has a lifespan, how the quality of digital content degrades with newer and faster technologies, and whether or not we are comfortable with what this means for the future of our content.<br><br><b>Background</b><br>We studied audio distortion techniques to decay the audio.  We also studied particle systems, physics, openGL, and meshes for the visuals.<br><br><b>Audience</b><br>\n          \t\t\t\tEveryone.\t\t\t\t\t<br><br><b>User Scenario</b><br>\n          \t\t\t\tA person walks by the piece and is intrigued.  He/she picks up the receiver at one end and speaks into it.  The audio gets transmitted and transferred from one screen to the next.  Another audience member walks by and pieces up the receiver at the other end to hear the final message.\t\t\t\t\t<br><br><b>Implementation</b><br>\n          \t\t\t\tThe audio is being transfered to each computer through its audioIn and audioOut jacks.  We're using openFrameworks to process the sound and output it.\t\t      \t\t<br><br><b>Conclusion</b><br>\n          \t\t\t\tWe both learned a lot by working on this project - but we think the greatest reward will be to see if our viewers/users learn anything themselves through the piece."},{"pitch":"George is your art companion. Part mobile and web application, George remembers what you found interesting at an art museum. Using the photos you take in gallery, George seeds your journey through the museum.","author":"Miguel Bermudez","title":"George, an Art Companion","url":"http://itp.nyu.edu/shows/spring2012/george-an-art-companion/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335804236_itpthesiswebsite.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tHow can we make paintings more engaging? What are the underlying stories between paintings that link them together? George was designed to address these questions.<br> George is a combination mobile and web application for use in art museum environments. The mobile component is designed to capitalize on the initial curiosity museum visitors have that compels them to take a photo of a painting. By taking the photo and leveraging image recognition technology, George's mobile component orchestrates your path throughout the museum. The paintings you visit along the way can be bookmarked for later. The web component comes into play after the visit to the gallery is complete. Each painting you've photographed and bookmarked is displayed here for you to explore further. George allows to you see which paintings you missed or dive deeper into the ones you visited."},{"pitch":"In this interactive floor game, you must stomp on dancing monsters.","author":"Dollee Bhatia","title":"Stompsters","url":"http://itp.nyu.edu/shows/spring2012/stompsters/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335822121_submit.png","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tThis is an interactive floor projection game that tests dexterity by requiring the user to stomp monsters on the floor.<br><br><b>Background</b><br>I took forward a project I found online by ex ITP student Liangjie Xia and I wanted to take it forward in an interactive game that fit perfectly for my classes Nature of Code and Biomechanics for Interaction.<br><br><br><br><br><br><br><br><br><br>\nI applied the concepts we learned in nature of code for the animation and movements of the monsters and incorporated user interaction using the Kinect. The final product is an interactive floor projection in which the user is required to stomp on dancing monsters.<br><br><b>Audience</b><br>\n          \t\t\t\tIt's a simple game that I'm hoping has a generic appeal to both kids and adults.\t\t\t\t\t<br><br><b>User Scenario</b><br>\n          \t\t\t\tIdeally the user would stand by the projection on the floor, the application would take a couple of seconds to auto-calibrate and then the user stomps on the projected floor monsters.\t\t\t\t\t<br><br><b>Implementation</b><br>\n          \t\t\t\tComputer with Speakers, Processing, Kinect and a piece of floor.<br><br><br><b>Conclusion</b><br>\n          \t\t\t\tLearned: how to learn and apply laws of physics in ways I could never imagine. <br><br><br><br><br>\nDiscovered: No one else attempted skeletal tracking data for floor projections<br><br><br><br><br>\nBroke: My fist.. this was a very frustrating process to get to a point where it worked."},{"pitch":"A simple test to help catch the exponential rise of a new kind of unknown pregnancy.","author":"Sheiva Rezvani","title":"Immaculate Pregnancy Test","url":"http://www.sheiva.com/itpblog/2012/04/30/immaculate-pregnancy-test-ipt/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335818267_immaculatepregnancytest.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tA satirical look at how pregnancy is defined by state law from state to state.  This project begins with a simple medical test where to determine whether the \"patient\" is pregnant.  If deemed so (as will be the case in most instances), then users will be directed to an interactive installation (a projection onto a chalkboard and mannequin) where they can learn more about their pregnancy."},{"pitch":"Experience cherry blossom viewing indoors.","author":"Melissa Dela Merced","title":"Scnery Project: Hanami","url":"http://scnery.com","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335763197_hanami.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tThis is a kinect based setup that will display a different image based on the location of the user in a given space. Images can vary from pictures to video to web camera based images<br><br><b>Background</b><br>Cherry blossoms is an interesting concept of the fleeting moment of great beauty. It's full potential is but a fleeting moment and the walk through the space symbolizes that. <br><br>\nI spent a few days at the Brooklyn Botanical Gardens just as the cherry blossoms reached full bloom. These only lasted for three weeks and I was fortunate enough to experience them before the rain came.<br><br><br><br>\nEventually I would design this with IP cameras focused on different places around the world to enable a \"window\" to the world that can reside anywhere.<br><br><b>Audience</b><br>\n          \t\t\t\tAnyone who wants to experience or enjoys the feeling of cherry blossom viewing.\t\t\t\t\t<br><br><b>User Scenario</b><br>\n          \t\t\t\tUsing the Center of Mass code, based on a user's location within the area of the kinect, the images on the screen will change as you walk further away or nearer to it. At the same time it will play Japanese festival music.\t\t\t\t\t<br><br><b>Implementation</b><br>\n          \t\t\t\tUsing the Xbox Kinect mounted above the floor, this will track the user using the SimpleOpenNI library for Processing. A screen on the wall will act as a window type where moving from one spot to the other will change the image displayed.\t\t      \t\t<br><br><b>Conclusion</b><br>\n          \t\t\t\tProjecting on mist is very difficult. A lot of mist or fog is required to project images on to it. I may pursue this off the floor using an actual fog machine."},{"pitch":"A sound installation for two unconscious performers","author":"Johann Diedrick","title":"Grand Dream","url":"http://vimeo.com/41806034","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335837721_profile.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tAn EEG reader translates brain electrical activity from the first performer into sounds. These sounds are heard by the second performer, who is also wearing an EEG reader. The second performer's brain electrical activity, influenced by the sounds produced by the first performer, is translated into sounds and heard by the first performer. An audio mix is projected into the space for visitors to hear the performers' sounds. Two psyches engage in a neuroacoustic feedback loop with bodies as instruments taking part in passive conversation.<br><br><b>Background</b><br>The project began as an attempt to uncover whether or not the unconsciousness, as a site of creativity, could be tapped into externally by sounds. If so, could it be influenced in a way that could be perceived by others? Could one \"play\" with the unconsciousness like an instrument? Could two people perform together, like a duet, by hearing each others' sounds that were produced by their own brain waves? Such questions began my work on this project.<br><br><b>Audience</b><br>\n          \t\t\t\tSound artists, media artists, sleep researchers\t\t\t\t\t<br><br><b>User Scenario</b><br>\n          \t\t\t\tThe ideal scenario would have me presenting documentation, along with providing demos of using the EEG readers to produce sounds between two people.\t\t\t\t\t<br><br><b>Implementation</b><br>\n          \t\t\t\tThe project uses two EEG readers, custom software (Processing, Pure Data) and audio equipment to translate brain electrical activity into sounds, that are then heard by each other participant. Video documentation of a demo from the middle of the semester can be found here: <http:> For the show I will be showing final documentation of the performance/installation being done live.\t\t      \t\t<br><br><b>Conclusion</b><br>\n          \t\t\t\tI learned that this project began with many questions and unveiled even more. It was an investigation that produced even more investigations.\t\t\t      \t\n\t\t</http:>"},{"pitch":"Sensitive shoes remind viewers about the importance of maintaining proper posture and gait while donning those favorite high heels. Users will receive instant feedback on their joint movements through an accompanying processing sketch that tracks and visualizes joint movement and outputs an audio reminder to correct movement, or a motion generated digital sketch. Walking sketches can live in the “Joint Movement Gallery” or can be printed out and taken home.","author":"Hsiao-Wen Chou","title":"Cinderella’s Blissful Ball","url":"http://s230863404.onlinehome.us/itp_html5/HTML5_spring/final_layout/index.html","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1336410036_intro_logo.gif","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tThis project is motivated by the growing trend in super high heels. Improper posture in high heels can lead to knee stress and injury. The core pieces of Cinderella are force sensors that are placed inside of high heels: one at the heel and one at the ball of the foot.  Correct posture and movement in high heels would show that the foot is making first impact at the ball of the foot, and the second impact point would be at the heel of foot. When the user is stepping incorrectly in the shoes, a processing sketch will trigger an audio reminder to straighten up. When the user is stepping correctly, there is the creative reward of a motion generated artwork being produced by the same processing sketch. This sketch can be saved as a .jpeg file that can be uploaded on a website, or printed out."},{"pitch":"Processing sketch that generates typefaces using a genetic algorithm.","author":"Ann Chen, Danne Woo","title":"Galapagos","url":"http://itp.dannewoo.com/nature_of_code/galapagos/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335842916_itp_font.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tGalapagos is a both a typeface creating tool and generative font made with Processing.  We were interested in typography, evolutionary design, and genetic algorithms.<br><br><b>Background</b><br>We experimented with Dan Schiffman's examples with genetic algorithms, in particular the ones that mimic genetic evolution, the strength and fitness of particular traits.  We applied the processing library Geomerative to our sketch to access the points along the curve of the<br><br><b>Audience</b><br>\n          \t\t\t\tGraphic Designers, Typography designers, people interested in easily creating font from their existing resources.\t\t\t\t\t<br><br><b>User Scenario</b><br>\n          \t\t\t\tWe will have the program set up on an iPad.  User approaches iPad, directions on how to begin generating typeface will be clearly presented.  When user generates first evolution of the typeface, they have the option of either printing and saving what they've created or creating another generation.  The characteristics of the next font generation (color, shapes, size, etc.) can be determined by the user depending on how long they hover over each example.  The longer they hover over one, the higher ranked that letter's characteristics will be and the more likely the next generation will look like that character.  User saves the print and can email print to themselves.\t\t\t\t\t<br><br><b>Implementation</b><br>\n          \t\t\t\tGalapagos was made in Processsing and wil be presented on an iPad--which will act as a monitor-- and will be connected to a laptop.  Archivable prints of type made with Galapagos will be hanging besides the setup.  These prints will be roughly 11\" x 14\".  One of these prints will show a complete typeface from A-Z, and a few more will be more abstract examples.\t\t      \t\t<br><br><b>Conclusion</b><br>\n          \t\t\t\tWe learned a lot about generative design and how to use the Processing library Geomerative."},{"pitch":"A word association engine run on manufactured unpredictability.","author":"Mimi Yin","title":"Prosaic","url":"http://itp.nyu.edu/~yy600/blog/prosaic/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335227114_prosaic.png","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tA word association engine run on manufactured unpredictability.<br><br>\nhttp://prosaic.co<br><br><b>Background</b><br>I’m interested in highly structured improvisation techniques that provide performers with the freedom to make decisions, but within a framework that generates patterns and disruptions in a way that imparts arc and form to a fundamentally “made-up-as-it-goes” process.<br><br><br><br><br><br>\nI wanted to literally encode these techniques in a medium where I could experiment and tune the parameters of the structure in a pseudo-laboratory setting. However, I wanted the output to be something a human could experience and pass judgement on.<br><br><br><br><br><br>\nHence, Prosaic’s medium is the computer and it’s content is text.<br><br><br><b>Audience</b><br>\n          \t\t\t\tHigh School or older.<br><br><br>\nPeople into text.<br><br><br>\nPeople into Artifical Intelligence.<br><br><br>\nPeople into improvisation techniques.\t\t\t\t\t<br><br><b>User Scenario</b><br>\n          \t\t\t\tUsers can watch the algorithm run on it's own or use a control panel to see the individual word association modes in action: Rhyming, Alliteration, Synonymy / Antonymy, Repetition, Remixing, Scrambling and Elision.\t\t\t\t\t<br><br><b>Implementation</b><br>\n          \t\t\t\tPython, Javascript<br><br><br>\nWeb Browser<br><br><br>\nWeb Server<br><br>\nEquipment: Large Flatscreen Monitor, Mac Tower, Keyboard, Mouse"},{"pitch":"An interactive video triptych that is meant to feel like a dream.","author":"Dan Scofield","title":"The Red Chair: An Interactive Triptych","url":"http://www.danscofieldstudio.com","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335895899_thesisbookcrop2small.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tAn interactive video triptych in portraits. A visitor navigates the abstracted audio-visual narrative with their gaze. Each frame leads down a different path; enabling infinite permutations. The experience of displaced control alludes to a dream.<br><br><b>Background</b><br>The Clock (film) - Christian Marclay, Sans Soliel (film) - Chris Marker, Audio-Vision (text) - Michel Chion, Vertigo (film) - Alfred Hitchcock, Science Fiction Sessions (audio recording) - Ornette Coleman, Chinatown (film) - Roman Polanski<br><br><b>User Scenario</b><br>\n          \t\t\t\tUser enters the \"booth\" sits in the chair, puts on headphones, watches video for anywhere between 3-10 minutes.\t\t\t\t\t<br><br><b>Implementation</b><br>\n          \t\t\t\tThe Red Chair aims to create a cinematic experience that cannot be distinguished from a dream. An interactive video triptych in portrait mode, it is designed for a single visitor to navigate using their gaze. Multiple abstracted narratives across three screens are threaded together by the viewer into a single immersive, interactive experience. The viewer navigates through a series of moving images by choosing one of the three videos with their eyes. Each video depicts people, places and textures from the ‘spaces in-between’: transitory empty spaces and destinations without geographic reference. As viewers move through the interactive narrative of The Red Chair, they encounter nameless faces, unidentified textures and inanimate objects. Eventually unifying threads emerge, enabling each viewer to stitch together their own personal narrative out of endless possibilities."},{"pitch":"\"If walls could talk...\"","author":"Manuela Donoso Lamas, Monica Bate Vidal","title":"Secrets","url":"http://itp.nyu.edu/shows/spring2012/secrets/","image":"images/none.png","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tA collaboration between Mónica Bate and Manuela Donoso, Secrets is an experiential wall full of secrets.<br><br><b>Background</b><br>For the Sensor Workshop Class, we start testing conductive paint as a sensor.<br><br><br><br>\nAs the paint is resistive and conductive as many applications a ways to use it.<br><br><br><br>\nFor this project we are using the paint as a proximity sensor, as a connector and part of the coil speaker.<br><br><br><b>User Scenario</b><br>\n          \t\t\t\tA big wall, clean and silent space."},{"pitch":"Sombras is an interactive video installation that calls us to realize the beauty and  the impermanence of life","author":"Luisa Covaria","title":"Sombras","url":"http://itp.nyu.edu/~lfc267/blog/?p=369","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335804479_sombras.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tThe piece immerses the viewer in a beautiful day at a beach while they listen to a woman talking about the memory of the best day in her life. She remembers a boy playing, a woman dipping her feet in the water , a couple walking along the beach and an old lady wearing a red dress. She describes this basic scene with the most utter pleasure. The user is able to pan left to right on the scene and his or her shadow is reflected on the sand. As we listen to the woman our shadow slowly disappears it is then that we learn that the woman is about to die and she will miss enjoying the pure beauty of simple things in life.  She concludes by asking ‘what memory are you going to take with you?’<br><br><b>Audience</b><br>\n          \t\t\t\tEvery person who is willing to take six minutes to observe and listen\t\t\t\t\t<br><br><b>User Scenario</b><br>\n          \t\t\t\tThe ideal scenario places the video instalation in a corner in a room. The user stands with his/her back towards a wall and faces the screen hanging from the ceiling. The sensor used for the piece is the kinect, which will work best if it is placed under the screen facing a wall. This set up prevents f the kinect from detecting  motion of people and objects that are not part of the experience.\t\t\t\t\t<br><br><b>Implementation</b><br>\n          \t\t\t\tThe video installation requires,<br>\none laptop or imac with processing and open NI, set of speakers, one short throw infocus projector, ceiling mount for projector, vga cable and extension, one kinect with usb extension, 4 feet screen (which is already made)needs to be hung  from ceiling."},{"pitch":"The recently released Happiness Index is mapped into sugar amounts for cupcakes.","author":"Annelie Berner","title":"Sweet Data: Happiness Index, in cupcakes","url":"http://itp.nyu.edu/shows/spring2012/sweet-data-happiness-index-in-cupcakes/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335888089_p1110037.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tTaste the data!<br><br><br><br><br><br>\nThe Cupcakes Index is a project that maps the country-by-country data of the Happy Index to amounts of sugar then added to the batter for country-by-country cupcakes.<br><br><br><br><br><br>\nAfter trying a cupcake, the participant maps the sweetness of the cupcake on a scale of Super Sweet to Not So Sweet. A live feed of this action is projected onto the formal graph of each country's place in the Happiness Index. Finally, there is brightness tracking that highlights the most recently added cupcake liners.<br>\nThis Happy Index, published in April 2012 by the Earth Institute, was contained in the World Happiness Report, co-edited by Jeffrey Sachs, that was commissioned by the United Nations Conference on Happiness.<br><br><b>Background</b><br>I considered the World Happiness Report, and specifically the Happiness Index that was just put out by Jeffrey Sachs, John Helliwell and Richard Layard at the Earth Institute. <br><br><br><br><br>\nI was interested in stimulating my audience to participate in the experience of making sense of data as well as incite curiosity about the methods of measuring happiness-- something that seems too subjective to measure and yet could be such an important way to develop policy. <br><br><br><br><b>User Scenario</b><br>\n          \t\t\t\tThe flow of the project is the following:<br><br><br><br><br>\nChoose two cupcakes (or more) from the index cabinet of cupcakes. They are not in order, though they are color-coded by continent.<br><br><br><br><br>\nTaste the amount of sweetness as it compares  to the second cupcake. The bottom of the cupcake wrapper reveals which country you were \"tasting.\"<br><br><br><br><br>\nTake the wrappers and pin them to a board according to your perception of their sweetness. <br><br><br><br><br>\nThe image of this board, a collaborative data representation, is then projected onto a third board that shows the original graph of the Happiness Index data set.<br><br><br><br><br>\nI will provide cards to those who are interested in understanding more about my experiment as it related to the Happiness Index. <br><br><br><br><br>\nIn this way, my participants will experience the dataset, the idea behind the dataset and its collection, as well as (hopefully) consider some of the driving questions that surround this area of work."},{"pitch":"An Every Day Object made by combining code, rhino 3d modeling and digital fabrication tools.","author":"Deqing Sun, Emily Webster, Engin Ayaz, Filipa Tomaz, Johnny Lu, Joseph Lim, Lilia Ziamou, Marcela Godoy, Matthew Epler, Michell Johanna Cardona, Oya Kosebay, William Jennings","title":"Sculpting Data Into Everyday Objects","url":"http://itp.nyu.edu/classes/sdieo-spring2012/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335888903_sdieocropped-white_random_1_lighter2.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tWe will be presenting projects we made as individuals or as groups. There are 9-10 projects ranging from wearable every day objects to sculptural light pieces.<br><br><b>Background</b><br>Class work for Sculpting Data. We learned rhino, processing, python, panelling tools in rhino, etc...<br><br><b>Audience</b><br>\n          \t\t\t\tAdults\t\t\t\t\t<br><br><b>User Scenario</b><br>\n          \t\t\t\tAdult walks up views work maybe carefully holds it in hand with student over seeing the work.\t\t\t\t\t<br><br><b>Implementation</b><br>\n          \t\t\t\tMost of the projects are made of printed plastic. Others are made of Z Corp powder prints while others are soldered wire that are covered in plastic dip.\t\t      \t\t<br><br><b>Conclusion</b><br>\n          \t\t\t\tThat one must iterate and iterate... only in this process can one's design evolve."},{"pitch":"An EMG sensor collects data from arm muscle contraction, activating the movement of a robotic, painting arm.","author":"Claire Mitchell","title":"Extension","url":"http://itp.nyu.edu/shows/spring2012/extension/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1336595424_4.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tAn attempt to merge the gestural movement in modern art, and the use of mechanized systems to create art. This piece uses the person's anatomical movement to control a machine, to create a drawing. The piece brings up questions of agency and control in creativity. A secondary exploration of the project will seek to use a similar concept and technology as a therapeutic tool in cases of musculoskeletal disfunction.<br><br><b>Background</b><br>I researched EMG circuits and projects from Advancer Technologies, as well as a previous project by ITP students Ezer Longinus, Johnny Lu and Alex Dodge.  I looked at work by other artists who use the gestures and body to create their work like  Pollock, Tony Oricco. I also looked at various robotic drawing machines, and artists that have combined the two- I recently found Stelarc's Third Hand Project. I did some brief research into the use of EMG and robotics in motor therapy for stroke patients- a scratch of the surface which has inspired me to continue research for future projects.<br><br><b>Audience</b><br>\n          \t\t\t\tPeople interested in biology, anatomy, mechanics, robotics, art, creativity and the intersection of all.  Anyone who sees promise in creating a mechanized system that uses muscle movement to become activated. The piece could be used as a tool to encourage the exercise of underused muscles in someone who lacks mobility.\t\t\t\t\t<br><br><b>User Scenario</b><br>\n          \t\t\t\tFor the show, there would be a video demonstration. The user would be shown wearing the EMG electrodes, when they flex their bicep, on link on the mechanized arm moves one axis. A second link on the arm is controlled by a flex sensor on the finger. This controls the other of the arm. At the end of the arm, charcoal makes marks on a vertical canvas. There would also be a table with the physical robotic arm, and EMG circuit / electrodes, and the canvas of artwork from the EMG controlled arm.\t\t\t\t\t<br><br><b>Implementation</b><br>\n          \t\t\t\tEMG circuit, electrodes. Robotic arm is made from a discarded disassembled desk lamp, two servo motors.\t\t      \t\t<br><br><b>Conclusion</b><br>\n          \t\t\t\tAmplification, sensors, EMG, servos, desk lamp construction, servo mounts. I'm looking forward to researching how to make this project more robust and to find practical applications for using EMG to control mechanized systems."},{"pitch":"An interactive photo booth that let's you show, using your hands, how much you love Red, printing it for sentiment, digitizing it for posterity.","author":"Lily Szajnberg","title":"I","url":"http://itp.nyu.edu/shows/spring2012/i/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1336697830_red3.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tUsing the Kinect, the user can change the size of a heart, which then appears between their hands. The user can see themselves and the heart juxtaposed over a backdrop of a photo of Red. The user then takes a picture of themselves, and the whole scene is printed on a 4X6 photo. The photo is also uploaded to Flickr."},{"pitch":"Simultaneously create a unique painting with a unique music composition by touching and exploring a blank canvas","author":"Ryan Viglizzo, Veronika Dubrovskaya","title":"Grid Mate","url":"http://itp.nyu.edu/~vvd212/blog/","image":"images/none.png","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tWe developed a framework in Processing and Arduino to create a musical/painting composition. The core idea is simple–place a sensor on a physical space (wall, ground, desk, etc.); create a visual of your choice, upload that visual to processing, project the processing sketch canvas on that space and attach sounds to your sensors. As you press the sensors your visual moves to the sensor that you pressed as well as triggers a sound that can be looped or activated once. There are four aspects to this project: “sensors”, “visuals”, “sounds” and ‘space’."},{"pitch":"How did your life end up going down this path?  “Between You &amp; Me” is an interactive video installation exploring this question.","author":"Tali Blankfeld","title":"between you &amp; me","url":"http://www.taliblankfeld.com","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1336064302_talithesisimageupdated.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tMajor decisions we make in our lives can lead us down various paths. This installation leads us through the journeys of individuals who have made meaningful sacrifices in their lives. With these stories, the installation asks questions about how our identities are formed and what role external circumstances have in the directions our lives have taken. Blending personal events with recurring answers demonstrating patterns in the adult life cycle, \"between you &amp; me\" connects us to one another in moments of our greatest struggles as human beings.<br>\nWith “Sophie’s Choice,” William Styron wrote about a mother facing one of the most heartbreaking sacrificial decisions known to man–having to decide which of her children would live while the other would die. As we see in the film, her life has been forever changed as a result of having to make this choice.  By looking at people who have faced similar, critical moments where they’ve decided to sacrifice something and learning more about how their lives have been changed as a result, \"between you &amp; me\" explores the weight of these decisions and their effects on our lives. In examining the outcomes of these sacrifices, we become aware of how major decisions such as these shape our lives, leading us down a path we might never have known.<br><br><b>User Scenario</b><br>“Between You &amp; Me” is discovered through a series of curtains.  Individual participants must pull one curtain aside at a time to watch the video projection on the following curtain; the video will play once the previous curtain is pulled.  Each video segment lasts between 20-45 seconds and consists of a documentary-style interview.  One person can enter the installation at a time to view it individually and will be asked to put on a pair of wireless headphones in order to listen to the audio that accompanies the video.  Along with the projection, each curtain has several phrases handwritten on it, taken from answers out of interviews submitted via questionnaire format online.  Interviewees consisted of people ranging from ages 22-70 years old, from all over the world, who submitted their answers anonymously.  Questions from interviews are available for users to read behind the final curtain."},{"pitch":"An immersive virtual reality sculpture, inhabited by endings.","author":"Catherine McCurry, Ivana Basic","title":"Reenacting the Fall","url":"http://itp.nyu.edu/shows/spring2012/reenacting-the-fall/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1336025386_helmet.png","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tParticipants, wearing a custom helmet, experience a virtual reality containing reconstructions of performative moments. Using sensor data from a mobile phone, 3d modeling software, and a sculptural helmet that blocks external light, we recreate real world motion in virtual space. This space contains figures caught in suspended motion which they reenact, obsessively, slowly."},{"pitch":"Plinko Poetry is a new interactive interface to design electronic poetic text.  Every player is a winner!","author":"Deqing Sun, Inessah Selditz","title":"Plinko Poetry","url":"http://inessah.com/portfolio/plinko-poetry/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1336063163_img_0092_700x640.png","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tPlinko Poetry is a new interface for electronic poetic expression. Drawing source text from current @nytimes and @FoxNews tweets, players can absurdly re-contextualize news headlines that are often overloaded with meaning. With roots in The Price Is Right and experimental blackout poetry- every player can be both a winner and a poet!<br><br><br><br><br>\nThe interface of Plinko Poetry uses Processing to display alternate scrolling lines of current tweets from the New York Times and Fox News. When a user drops a chip, it randomly hits pegs on the way down. The word under each peg that is hit is highlighted, with the untouched pegs automatically darkened. Plinko Poetry uses openFrameworks camera color tracking to determine which pegs have been encountered. When the chip comes to a stop, the user is left with a trail of blackout poetry which is then live tweeted to @PlinkoPoetry.<br><br><br><br><br>\nUltimately users will create a new corpus of ever changing poetic text based on the zeitgeist of current headlines.<br><br><br><b>Audience</b><br>\n          \t\t\t\tAnyone who likes weirdo poetry and has eyes?\t\t\t\t\t<br><br><b>User Scenario</b><br>\n          \t\t\t\tPlayer walks up to screen where text is scrolling. Player drops chip and text either highlights or blackouts depending on if the chip touched the corresponding peg. User is left with a trail of blackout poetry. Poems are then live tweeted to twitter and shown on a separate display.\t\t\t\t\t<br><br><b>Implementation</b><br>\n          \t\t\t\tIt's made of a LCD screen incased in layers of CNC'd wood and plexiglass.\t\t      \t\t<br><br><b>Conclusion</b><br>\n          \t\t\t\tA lot, hopefully it doesn't break!"},{"pitch":"REMQuilt helps couples decipher the secret language of their sleep. Each night, it tracks their movement, position, and body contact; and in the morning, it weaves this data into a representation of restlessness, proximity, and touch that is projected on their bed in a quilt-like pattern.","author":"Amelia Hancock, Justin Lange, Patrick Muth","title":"REMQuilt","url":"http://itp.nyu.edu/shows/spring2012/remquilt/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1336599158_remquilt_for_show.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tREMQuilt helps couples decipher the secret language of their sleep. Each night, it tracks their movement, position, and body contact; and in the morning, it weaves this data into a representation of restlessness, proximity, and touch that is projected on their bed in a quilt-like pattern.<br><br><br>\nREMQuilt is meant for a limited audience-- the visualization is intended for couples. Because of the intimacy associated with the context of this data, we chose an abstract graphic approach-- both to mimic the abstraction of traditional quilt squares, as well as to respect to the couple's shared secret data.<br><br>\nWhen viewed from the side of the bed, rows of quilt squares read left-to-right, creating a grid that is representational of a possible nine hours of sleep. Each individual 'quilt square' itself represents two-and-a-half minutes, accounting for each partner's time, creating an x axis of 24 quilt squares (or an hour of sleep). The y-axis accounts for nine hours for each couple, represented by a total of 18 quilt squares.<br><br>\nThe data is represented graphically in both geometric shape and color. To represent movement, we have chosen two horizontal 'jagged' lines, that, when viewed within the context of the quilt, create visual noise, making a clear distinction between the data and visual cues. In contrast, to represent a lack of movement, we have chosen two 'straight' horizontal lines, which achieve the opposite effect, creating a visual, calming sense of 'still'. Because couples may be restless or still while touching, we chose to simply unite the two representational 'movement' lines into one to visualize periods of contact between the couple. Finally, we chose color to represent proximity, which is represented by 'heat scale' values--- blue, a cool color, indicates when the couple is far apart, orange, a little warmer, represents a closer proximity, and finally, red, a warm color, represents the closest proximity.<br><br>\nThe REMQuilt user interaction is quite simple. The first partner to get into bed interfaces with either a desktop or mobile application, simply inputting the time that the latest-sleeping partner wishes to wake up, as well as indicating that they are starting their evening of rest. REMQuilt tracks time, and when the wake-up time is reached, REMQuilt will trigger soft music and bring the projection to light to be viewed.<br><br>\nREMQuilt is comprised of a number of digital textiles: a Kinect camera for low light camera and depth tracking; libraries in openFrameworks to suggest movement and proximity; two conductive pillow cases and an Arduino to measure body contact between sleepers; a laptop computer and software to knit together all of these variables, and a projector mounted above the bed to illuminate the connections in the morning."},{"pitch":"Create a conversational space between the men’s and women’s restrooms using an \ninteractive grafﬁti wall.","author":"Atif Ahmad, Bruna Calheiros, Matt London, Michael Gambale, Patrick Muth","title":"Text The Throne","url":"http://www.textthethrone.com","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1336064798_textthethrone.png","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tText The Throne is an installation meant to create a conversational space between two groups, in a place where they wouldn't normally communicate, the bathroom. Using a combination of SMS, Processing and projectors, we have a created a space where men and women can communicate in the bathroom in an anonymous fashion.<br><br><br>\nIn both the men's and women's bathroom near the admin offices, our group installed a projector above the toilet of one of the stalls. Connected to a Mac Mini, the projector displays a Processing sketch onto the wall. Upon entering the stall, users are greeted with a sign, encouraging them to submit a message via SMS that will be displayed on both bathroom walls. Within seconds of being sent, the message is added to the Processing sketch, which is stylized to resemble hand written graffiti.  <br><br><br>\nWe purposefully gave our users the bare minimum of a prompt, because we wanted them to decide how and where the conversation went. In our brief testing, Text The Throne generated conversations that featured direct responses between users, as well as those that generated humorous memes. Being in the show would be an opportunity to test our project with non-ITPers (a.k.a. normals) and see how they would react to this unique installation."},{"pitch":"Celluloid is an installation where, armed with a flashlight, one can explore films hidden in 16mm film segments.","author":"Adria Navarro-Lopez, Luisa Pereira Hors","title":"Celluloid","url":"http://itp.adrianavarro.net/wp-content/uploads/2012/04/Celluloid-design.pdf","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335758695_img_3152.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tA frame hangs in a dim room. As a visitor comes in, she picks up a flashlight. She is able to make out the frame, and points it in that direction. She can then tell the stripes in the frame are made of film, and gets closer to inspect them. There are studies in black and white, sci-fi scenes, sepia frames, car explosions, animated characters. Sporadically, A flash of light runs through one or another stripe. She shines a light on one of them, and then the room lights up: a movie is projected on the frame, on the films themselves. She moves the light up and down one segment of film, and the movie moves forward or backwards. Flashing a light on another one reveals a different movie, which she controls with her light as well. <br><br><br><br><br><br><br><br><br><br><br><br>\nThe idea for this project came from our interest in mixing digital and analog media, defining a space using a simple material (such as tape), and working with a material on the verge of obsolescence, and that people do not normally have contact with (such as film). The design of the frame –the segments of unrelated films that trigger somewhat related segments; the control the user has over what happens, and the fragmented experience– could be read as a reference to the way we experience video today. We are curious to see what people make of it (and what we make of it, once it’s finished)."},{"pitch":"An analog camera for your digital life.","author":"Adria Navarro-Lopez, Ananya Mukherjee, Dong Ik Shin","title":"Polaroid Cacher","url":"http://itp.nyu.edu/shows/spring2012/polaroid-cacher/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1336143023_dsc_5161-2-show.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tEveryday we spend more time in the virtual world and less on the physical. We are no longer defined by our physical appearance and acts, but also by what we project of ourselves on the internet. Important conversations, discoveries and encounters occur there, but most of them are forgotten, lost under layers of information, databases and outdated services.<br><br><br><br>\nPolaroid and other kinds of instant photography are more associated to memory than photographic quality. For this reason, our camera takes actual polaroid pictures of digital moments, in order to create tangible mementos of them."},{"pitch":"an interactive lighting installation for collaboration","author":"Sukmo Koo","title":"Hand in Hand","url":"http://itp.nyu.edu/shows/spring2012/hand-in-hand/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1336117275_574862_650257975815_17402347_33014654_1763106012_n.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\t1. people will connect physically with the installation for collaboration <br>\n2. This installation will make people to join a movement to help others.<br><br><b>Background</b><br>1. Marieb, E., Hoehn, K. Human Anatomy and Physiology. 7th Ed. 2007. Pearson Benjamin Cummings: San Francisco.<br>\n2. Saladin, Kenneth (2007). Anatomy and Physiology: The Unity of Form and Function. McGraw Hill. p. 544-546<br>\n3. 5 Problems with Social Networking in the Workplace by David Kelleher /communications and research analyst at GFI<br>\n4. Hand in Hand ,Koreana - 1988 Seoul Olympic song<br>\n5. Design for Unicef<br>\n6. Semaul Undong Movement in South Korea.<br><br><b>Audience</b><br>\n          \t\t\t\teveryone\t\t\t\t\t<br><br><b>User Scenario</b><br>\n          \t\t\t\t1. two people or more standing in front of the installation. 2. each person grabs a fake hand on the installation.3. people grab each hands next to another or others. 4. blinking LED light, move to center point, heart beating sound turn on. 5. when two LED cylinder meet at center point.- full brightness with Kissing sound.\t\t\t\t\t<br><br><b>Implementation</b><br>\n          \t\t\t\tHand in hand, the interactive installation can help people to connect each other emotionally by their hands for collaboration. We can have an opportunity to identify our forgotten emotional feelings again with this interactive installation.\t\t      \t\t<br><br><b>Conclusion</b><br>\n          \t\t\t\tI learned how to build the prototype and also think about why people need collaboration in these day. this lighting can help people to think why human being needs friends<br>"},{"pitch":"This project is about an interactive, entertaining phone experience to lift a house up through collaboration.","author":"Ji Hyun Lee","title":"UP!","url":"http://itp.nyu.edu/~jhl589/myblog/portfolio/up/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1336517837__up.png","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tThis project is about an interactive, entertaining phone experience through collaboration. The UP! gives people a virtual experience to lift a house up just like Disney/Pixar's Up animation. Users can easily participate in the UP! by phone and grow their balloon by blowing on the phone. Not an individual can do it but it comes true through collaboration.<br><br><b>Background</b><br>One of my favorite part of my project is collaboration. The collaboration maximizes satisfaction because everyone can be a contributor to do something. There is a research that uncovered participatory experiences make people feel needed. Furthermore, there is a saying \"A dream you dream alone is only a dream. A dream you dream together becomes reality\" quoted by John Lennon.<br><br><b>Audience</b><br>\n          \t\t\t\tPeople of all ages who have a phone!\t\t\t\t\t<br><br><b>User Scenario</b><br>\n          \t\t\t\tPeople are supposed to call a designated phone number and blow air on their own phone to grow their balloon. Each balloon has the last 4-digit of phone number so that users can see which balloon is theirs. The house starts to be lifting up in the air when sum of the air of balloons is enough.\t\t\t\t\t<br><br><b>Implementation</b><br>\n          \t\t\t\tThe UP is implemented by Processing and Box2D. Mostly, I was playing with parameters in the Box2D world to make balloons looking like real. In addition, I use a Tinyphone server which has an IP address and port number so that the designated phone number can be linked to the network. Each phone call can be a client giving events such as audio levels and key press. I interpreted the audio level as an air that the user blow on the phone.\t\t      \t\t<br><br><b>Conclusion</b><br>\n          \t\t\t\tI learned that Box2D is a great library which helps me mimic what is happening around our lives. I found phone can be a good, mobile tool for the user interaction and phone experience to be fun and enjoyable."},{"pitch":"The Wiki-History interface allows users to easily view all past edits of a Wikipedia article on a single page. It's based on the idea that an article's edit history contains a conversation which is vital to understanding the article's topic.","author":"Kate Tibbetts","title":"Wiki-History","url":"http://itp.nyu.edu/~kt922/coll_storytelling/final_wiki/wiki_d2.html","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1336753924_wiki.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tWiki-History is an interface which allows a user to easily scroll through every edit of a Wikipedia article, thus exposing a conversation within the edit history. The result is a teaching and research tool that resists the tendency to narrow history into an overly-simplified narrative, while remaining clear and searchable. <br>"},{"pitch":"Artphones provide the framework and inspiration to turn any physical form into a working cell phone or walkie-talkie.","author":"Bobby Genalo","title":"Artphones","url":"http://itp.nyu.edu/shows/spring2012/artphones/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335130332_genalo_onlinethesis_image.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tIn the age of customization, I often wondered why I couldn’t have more of a say in what my cell phone looked or felt like.  Intrigued, I walked around the city with a piece of wood held to my ear, pretending to hold a conversation as passersby looked on in bewilderment.<br><br><br><br>\nThis playful, abstract way of viewing the world is one that I am looking to foster both in myself and in others.  During the semester I worked with a third-grade class in Brooklyn to develop custom walkie-talkies and, with adults, discussed the hidden benefits of designing one’s own screen-less, internet-less cell phone (e-waste consciousness, personalization, et al).<br><br><br><br>\nMy hope is that Artphones will encourage people to blur the line between producer and consumer, empowering them to ask more questions and lead more creative, responsible lives."},{"pitch":"A 100 square foot interactive mirror made up of hundreds of unique neon symbols.","author":"Avery Max","title":"Neon God","url":"http://itp.nyu.edu/shows/spring2012/neon-god/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335738360_symbol_map_900.png","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tThe neon mirror is 10'x10', and holds a 20x20 grid of neon symbols. There are 400 unique symbols total which are categorized by color and theme: Red (Evil), Orange (Love/Care), Yellow (Happiness/Comfort), Green (Nature/Wonder), Blue (Emptiness/Superficiality), Purple (Soul/Creation/Sexuality), and Pink (Fun/Childhood). It creates an enormous neon rainbow that is symbolically reflecting the experiences of life while physically reflecting the viewer of the piece.<br><br><br><br>\nThe symbols were drawn in Illustrator as paths and then CNCed as grooves into panels of wood. EL Wire is then run through each shape and through a terminal hole in the panel, where it connects to a custom designed circuit board that allows for control of each of the 400 symbols individually.<br><br><br><br>\nA Kinect camera embedded in the center of the mirror sends a video feed which is processed in order to determine which symbols to turn on/off to create a \"reflection\" of the viewer of the piece.<br><br><b>Implementation</b><br>\n          \t\t\t\tThe piece is constructed of Marine Grade Plywood which has been primed and painted semi-glossy black. The piece is segmented into twenty-five 2'x2' panels which come together in a 5x5 grid to make the 10'x10' footprint. A series of U-Plates and bolts/nuts hold the panels together.<br><br><br><br>\nElectronically, the piece is powered by 25 custom designed circuit boards which can each control 16 strands of EL-Wire. The boards daisy chain together (using a buffered I2C bus) and the entire piece is connected together with a simple 4 wire interface.\t\t      \t\t<br><br><b>Conclusion</b><br>\n          \t\t\t\tThis is the biggest project I have ever attempted and I learnt EVERYTHING. I learnt about physical fabrication, custom PCBs, I2C, controlling AC vs DC, the CNC machine, general woodworking, and most importantly - how to fully finish a massive project."},{"pitch":"Through the global dispersion of small artifacts, the Timeseed Project is an endeavor to transmit the reality of one person's values and existence to the inhabitants of the future.","author":"Jackson Snellings","title":"The Timeseed Project","url":"http://www.timeseedproject.org","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335040111_screen-shot-2012-04-21-at-4.23.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\t<br><br><b>Background</b><br>This project made extensive use of 3D modeling software.  This was most frustrating as had no useable experience in the subject prior to this attempt. Ultimately I developed a method that was able to produce a satisfactory result.  I then had the files printed in 3D using the objet liquid plastic printer.  The resulting prints were very high resolution and were used to make high quality rubber molds.  Plastic copies were made from the rubber molds in order to make Hydrostone production molds while not risking the 3D originals to water exposure.  After some experimentation with mold-making, I was able to create a series of functional molds that I have used to create nearly 100 copies to date.<br><br><b>Implementation</b><br>\n          \t\t\t\tI have, through 3D printing, mold-making, and ceramics created a series of ultra-durable porcelain coins.  One side of all of these coins features a 3D bais relief of my face generated via kinect, a personal statement, and a small depression that will contain a small amount of my DNA preserved in plastic. on the alternate side one of several pictographic layouts on the subjects of: Solar Order, the Golden Ratio,  Tiennamen Tankman's non violent resistance, evolution, are displayed."},{"pitch":"Barcode Bots is a game that encourages exploration of the physical world by transforming the ubiquitous UPC barcodes found all around us into unique, dynamic creatures that can then be used in multiple face-to-face gaming contexts.","author":"Nick Santaniello","title":"Barcode Bots","url":"http://www.barcodebots.net","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335159745_photoshopscreensnapz004.png","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tBarcode Bots is a mobile game where players collect unique creatures (called Barcode Bots) by scanning UPC barcodes with their mobile device. Each \"Bot\" is given a unique appearance and properties (such as \"strength\", \"intelligence\", \"sauciness\", etc.) based on the digits encoded in the barcode. The product scanned, as well as the player's geolocation are optionally recorded.<br><br>\nThese creatures are then used as avatars in secondary games (or 'contexts') where players can team up or pit their creatures against each other in different contests of skill. The first 'context' will be a multiplayer iPad racing game or a 'battle arena' tablet application. Other opportunities include hunting for hidden 'secret' creatures scattered throughout New York City, or building an online database of a player's scanned barcodes and Bots. <br><br>\nBarcode Bots encapsulates two of the most under-explored aspects of digital gaming: physical exploration and face-to-face social interaction."},{"pitch":"Pyramid Hill is a 3D virtual world with different environments to explore and characters to interact with.","author":"Matthew Rader","title":"Pyramid Hill","url":"http://pyramidhill.tumblr.com/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335486220_wubbaworld2small.gif","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tWelcome to Pyramid Hill. <br><br>\nPyramid Hill is your gateway to Reed + Rader’s virtual worlds. <br><br>\nThese experiences are fully interactive and are made from the finest cutting edge 3D engine, animation and 3D scanning technologies.<br><br>\nCurrently we have three experiences available to share:<br>\n1. Wubba World - Parachute out of a blimp through a cloud of balloons onto a rainbow colored landscape populated by a race of giant smiling ice cream cones. <br>\n2. Bots – Visit a post-singularity laboratory filled with experiments in human augmentation, dinosaur faces, and tentacles. <br>\n3. Cretaceous Returns - Return to the Cretaceous period and ride among friends with giant brontosauruses.<br><br>\nHaving trouble finding an experience that you like? We apologize, new worlds are in progress.<br><br>\nFor further communications: http://pyramidhill.tumblr.com<br><br>\nHave a good day.<br><br><b>User Scenario</b><br>\n          \t\t\t\tPyramid Hill is a virtual world that exists on a screen.  Users can \"walk\" around the environment and interact with various characters by using the analog sticks on a game controller.\t\t\t\t\t<br><br><b>Implementation</b><br>\n          \t\t\t\tPyramid Hill is built using the Unreal Development Kit which is a toolkit for working with Epic''s 3D Unreal Engine. <br><br>\nAsset Creation:<br>\nTextures, Animations, Movies: Made in After Effects and Photoshop<br><br>\n3D Asset Creation:<br>\nScanned using 123D Catch photo stitching and Kinect scanning with ReconstructMe."},{"pitch":"Semantic video editor for YouTube","author":"Stepan Boltalin","title":"Artificial Intelligence Video Editor","url":"http://www.videai.com","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335231109_pic2.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tNow that millions of hours of online video have been tagged and described by users, new machine-based new-content-creation software, VideAI, has become possible. VideAI is an Artificial-Intelligence-based video editor. A user inputs text and receives a video clip within minutes chosen by the machine to fit that text (the video-clip source is currently YouTube). A machine-learning linguistic algorithm analyzes the user's feedback over time to continually improve the best fit between text and video for future inquiries. VideAI uses YouTube’s metadata and comments to facilitate unsupervised production of new media objects. Combining natural-language processing with video-editing techniques, VideAI allows the user to collaborate with Artificial Intelligence in the creation of new media objects. In this way, the gap between text and image will become smaller and smaller."},{"pitch":"An evolution of the kinetoscope, in which the user unconsciously interacts with and alters the video.","author":"Elena Parker","title":"Walter","url":"http://bit.ly/walterkinetoscope","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335559645_walterlogo.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tExpanding upon the Kuleshov effect and Walter Murch's \"In the blink of an eye\" theory, I built an eye tracking mechanism into a kinetoscope. The eye tracker monitors your gaze as you watch the film, and every time that you you blink, you edit the video.<br><br><b>Audience</b><br>\n          \t\t\t\tFilm and television viewers, film theorists and historians, lay individuals\t\t\t\t\t<br><br><b>User Scenario</b><br>\n          \t\t\t\tA user will approach the kinetoscope and the video will start. As the user blinks the video cuts to different perspectives on a single scene. The user is unaware of his or her effect on the video. When the 2-minute video is over, the user can discover that they actually viewed a personalized film, either via a \"receipt\" with the time of their edits or via speaking with me directly.\t\t\t\t\t<br><br><b>Implementation</b><br>\n          \t\t\t\tThe kinetoscope is made using a mini-projector. The eye tracker is built with a hacked and customized PS3 Eye camera, ir reflecting mirror and ir LED. The camera detects the glint in your eye from the LED via the mirror -- whenever the glint is gone [the reflective parts of your eye are covered by your eyelid when you blink], the program cuts to the next shot of video."},{"pitch":"Data Viz of Republican voter approval rates in the form of sex toys.","author":"Matthew Epler","title":"Grand Old Party","url":"http://itp.nyu.edu/shows/spring2012/grand-old-party/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1336429591_profile_render.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tGRAND OLD PARTY is a set of 6 erotic toys made from voter approval data for each of the 2012 GOP Presidential candidates. Each piece's shape is a direct translation of data taken from Gallup's  Poll data from December 10, 2011 to April 1, 2012."},{"pitch":"This short interactive documentary is an attempt to explain to my daughter what it was like being born and raised in the Children of God cult until I left at age 20.","author":"Christina Arnold","title":"Growing up Absurd","url":"http://growingupabsurd.org/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1336436359_4childrenofgod.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tRecently, I told my teenage daughter the truth of my childhood – that I was born in the Children of God (CoG) cult. It was founded in 1968 by David Berg, a delusional Prophet of the “Endtime”. Thousands of hippies, like my parents, were its devoted first members. My daughter found the cult’s beliefs to be as absurd and insane as I did when I was the same age. Her reactions made me explore more deeply my own, unanswered questions. My father, who left the cult after 30 years – agreed to an interview about topics we had never discussed. The result is an intergenerational perspective on the CoG cult. Finally, I explain how comedians like George Carlin, Bill Mahr, Tim Minchin, Louis C.K. and others such as Christopher Hitchens, helped me reshape my worldview and influenced how I raise my daughter.<br><br><b>Audience</b><br>\n          \t\t\t\tThe general public above the age of 16.\t\t\t\t\t<br><br><b>User Scenario</b><br>\n          \t\t\t\tThe user will enter a curtained booth where they will fine a set of headphones to wear. Then they will enter the interactive website: http://growingupabsurd.org/<br><br><br><br><br><br>\nA short explanation of the project will be provided. and the user will be guided to enter the site. <br><br><br><br><br><br>\nThis is an interactive documentary about my experiences growing up in the Children of God cult. I am using this format to answer questions my daughter has asked about my childhood. <br><br><br><br><br><br>\nEach video clip answers a question. After each clip, the user will be presented with choices.<br><br><br><br><br><br>\nThe users choices will create a unique journey through this story. <br><br><br><br><br><br><b>Implementation</b><br>\n          \t\t\t\tThis project is the result of dozens of original and edits from other relevant videos, They are cut into clips and arranged by themes, which are accessible as the user travels through the many levels of the site.\t\t      \t\t<br><br><b>Conclusion</b><br>\n          \t\t\t\tI learned a great deal about myself, my own unresolved questions and the critical importance of documenting my answers to my teenage daughter's questions in a format/medium that she will most appreciate."},{"pitch":"Live visualization of a user's heartbeat.","author":"Phan (Pan) Visutyothapibal","title":"visualizing heartbeat","url":"http://www.phanv.com/blog/visualizing-heartbeat/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1336437243_heartbeat_vis_04ssm.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tThe user's heartbeat is analyzed using a stethoscope. The user's heartbeat is used to provide pulse rate and amplitude. A visualization using these two factors is projected for user to experience.<br><br><br><br><br><br><br><br><br><br><br><br>\n1) pulse rate of the user drives the speed of the animation. <br><br><br><br><br><br>\n2) the amplitude (or volume of the heartbeat  captured) determines the brightness of the visualization.<br><br><br><b>Background</b><br>I'm inspired by live sound and animation.  Prior to this I used sound that i designed or composed, but for this project I wanted live sound that reflects a slice of life of the user. <br><br><br><br><br><br><br><br><br><br><br><br>\nResearch: Using the concepts taught in nature of code, I explored MAX/jitter to learn more about how live amplitude of sound can manipulate 3D animation that appropriately represents the nature and depth of sound<br><br><b>Audience</b><br>\n          \t\t\t\tI would like to think that this has a generic appeal for everyone vaguely interested in how sound of their heartbeat can translate to animation... or in other words: Almost everyone.\t\t\t\t\t<br><br><b>User Scenario</b><br><br><br><br><br><br><br>\nI intend to use a stethoscope with a built in microphone that will pick up the pulse rate and amplitude of the heartbeat.  The users will then hear their own heartbeat and get to see visualizations.<br><br><br><br><br><br><br><br><br><br><br><br>\nTo make the experience border medical art, I will be dressed as a doctor and will treat users as patients.\t\t\t\t\t<br><br><b>Implementation</b><br>\n          \t\t\t\tStethoscope, Microphone, Mac with Max/Jitter, Projector /LCD screen.<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>\nAdditional Setup: Table, chair, and lollipops for successful visualizations.\t\t      \t\t<br><br><b>Conclusion</b><br>\n          \t\t\t\tLearn: More about jitter, how to import 2D objects into openGL 3D space. <br><br><br><br><br><br>\nDiscover: The simpler concepts are better.<br><br><br><br><br>\nBreak: Stethoscope to put a mic in."},{"pitch":"A selection of data visualizations including Insulin on Board, a representation of my diabetes medical data.","author":"Doug Kanter","title":"Visualizations","url":"http://dougkanter.wordpress.com/2012/02/28/itp-1st-data-rep-assignment/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335723820_visualizations.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tI am featuring my work from the Representation class. This includes my final project called Insulin On Board. As part of my personal project Databetes, I continue exploring new ways for patients to make better use of their personal medical data to improve their health. Insulin on Board examines the relationship between insulin totals and the resulting blood sugar levels.<br><br><b>Background</b><br>As a diabetic of 25 years, I believe that patients need better tools to manage all the data they are producing. I produce over 300 data points a day. There is no service on the market that handles this information effectively.<br><br><b>Audience</b><br>\n          \t\t\t\tPeople who are interested in data visualizations, big data and health care reform.\t\t\t\t\t<br><br><b>User Scenario</b><br>\n          \t\t\t\tI will print out the above projects and ideally hang them on the wall or from the ceiling. An additional monitor would allow me to also present additional work on the computer\t\t\t\t\t<br><br><b>Implementation</b><br>\n          \t\t\t\tThe Insulin on Board project will be one very long print, since it covers 100 days of data. I would probably need to tape these projects to the wall. I am scheduled to go to the AMS printer on Tuesday.\t\t      \t\t<br><br><b>Conclusion</b><br>\n          \t\t\t\tI learned that my diabetes control is much more likely to occur the less insulin I take. The data shows that a low-carb diet works."},{"pitch":"The Circuit Board uses the kinetic energy from a spinning skateboard wheel to produce power, lighting undercarriage LEDs.","author":"Danne Woo","title":"Circuit Board","url":"http://vimeo.com/38411898","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335649025_photo.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tThe skateboard wheel spins a motor that outputs up to 7 volts depending on the speed of rotation. The Circuit Board can charge anything that requires electricity. The possibilities are endless.<br>"},{"pitch":"Time Lapse Farm Ledger is a narrative that follows the introduction of Monsanto to Southeastern Michigan and its adverse effects on my family's health, our farm, and regional wildlife.","author":"Kawita Kandpal","title":"Time Lapse Farm Ledger","url":"http://itp.nyu.edu/~kk1985/blog/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335629597_farmledger.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tIn preparation for the foreclosure of our farm in May, I began the process of collecting our farm ledgers. The purpose of these ledgers is to record the day's accounting; however, my Father also employed the ledger as a diary and there were entries that gave me pause, as there were observations about Monsanto. Considering this, I retrieved seven ledgers from our home, one for each day of the week and selected a gestational stage to be represented in each ledger of a common field sparrow indigenous to Michigan. I am in the process of creating a space in each ledger as a place holder for the developing sparrow. I have extracted content from my Father's ledgers and these will be placed on the left hand page in proximity to a narrative about his interactions with Monsanto. On the right hand page, I will render seven stages of embryonic development of a sparrow in clay.<br><br><b>Background</b><br>I possess a background in biology and have always been interested in exploring a narrative about the life of the American farmer and the livelihood that may be evolving toward extinction.<br><br><b>Audience</b><br>\n          \t\t\t\tAnyone who is interested in farms, farming, and farm life from a first person point of view.\t\t\t\t\t<br><br><b>User Scenario</b><br>\n          \t\t\t\tI consider this piece contemplative and it is my hope the participant is able to engage with the narrative, while being present as a witness to a bird's birth. I am in the process of adding a projector to place video images upon the final gestation stage of the sparrow. The final stage consists of a baby bird with two heads.<br><br>\nThe person of interest whom I would like to highlight in the video is former Monsanto executive and current Deputy Commissioner for Foods at the FDA, Michael R. Taylor. Should video footage of Taylor be insufficient, I shall employ video of Monsanto representatives and farmers discussing product safety on the record.\t\t\t\t\t<br><br><b>Implementation</b><br>\n          \t\t\t\tThe installation is made of paper and clay. The gameplan is to offer a narrative of farm life and the livelihood that was ours.\t\t      \t\t<br><br><b>Conclusion</b><br>\n          \t\t\t\tWe possess complex attachments to a place. For myself, my life and livelihood are interconnected as there is a proximity that exists between the professional and personal, the economic and ecological."},{"pitch":"Who's looking at you, kid?","author":"Kim Ash","title":"Voyeur","url":"http://itp.nyu.edu/~ka1019/blog/voyeurism-redux/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1336461017_screenshot2012-05-08at3.09.20am.png","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tWhether we know it or not, we are constantly being watched. Sometimes we are under surveillance; other times our behavior is monitored to sell us something. What is public, and what is private? And what expectations do we have when entering a public space? Humanity has a culture of voyeurism. We love to people watch. The longer we look, the harder it is to look away. I want viewers to question their behavior on both sides of the screen. The blur boxes remind users that they are participating in an intrusion, and also allow them an opportunity to give privacy back to the people they are watching.<br><br><b>Background</b><br>The original version of this program connects to public (i.e. not password protected) IP cameras in 2 bars in Europe. The patrons of these bars are unaware that they are being broadcasted. While there is no expectation of real privacy when in a public space like a bar, there is something invasive about being watched by a stranger. The green-rimmed boxes blur into obscurity any people and objects underneath them. I want to remind users that they are participating in an intrusion, and also allow them an opportunity to give privacy back to the patrons they are watching. I would like to tweak this for the show so that the video feed would come from a camera on the floor pointed at ITP visitors.<br><br><b>User Scenario</b><br>\n          \t\t\t\tUsers walk around the floor, unaware that they are being watched. When they arrive at my project, they watch other people onscreen. Users are able to move around the blur boxes to obscure the faces and bodies of the people they are watching. In this way, users participate in both sides of the surveillance exercise. They may choose to be voyeurs or protectors of the people onscreen.\t\t\t\t\t<br><br><b>Implementation</b><br>\n          \t\t\t\t1 or 2 IP cams, an internet connection, and Eclipse/Processing. I would prefer a large screen."},{"pitch":"Feed Me! is an interactive game in which participants are asked to help an on-screen avatar complete a challenge by virtually and physically feeding a life-size model various types of food.","author":"Allison Berman, Blaire Moskowitz, Nelson Ramon","title":"Feed Me!","url":"http://itp.nyu.edu/shows/spring2012/feed-me/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1336491793_feedme2.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tFeed Me! is an interactive installation game in which participants are asked to help an avatar complete a challenge by virtually and physically feeding it different types of food.  The game is intended to teach the basic working principles of the human digestive system including building and deconstructing objects which represent food.<br><br><b>Background</b><br>After significant research to refine our project plan, we did alpha testing with classmates.  Next, we took the model to NYSci for phase one beta testing with middle school students and will return for a second round of beta testing this week.  We tested the knowledge of the children pre- and post game play and monitored the amount of time the children played the game.<br><br><b>Audience</b><br>\n          \t\t\t\tThe intended audience comprises of middle school science students who are starting to learn complex scientific concepts. The New York State Department of Education’s Intermediate Level Science Core Curriculum (Grades 5-8) states that students should understand, “The digestive system consists of organs that are responsible for the mechanical and chemical breakdown of food. The breakdown process results in molecules that can be absorbed and transported to cells.” This guided experience is apt for middle school students because they are mature enough to make their own choices while performing the pre-written challenges that the game or educator has chosen, which correspond to their level of classroom learning.\t\t\t\t\t<br><br><b>User Scenario</b><br>\n          \t\t\t\tChildren will be presented with an avatar on a touch screen.  The screen presents a nutritional challenge for the avatar to complete.  The child will select the food that will complete the challenge and directions to build the food out of blocks will appear.  Switching to the physical part of the game, the children will assemble a \"food compound\" and feed it to the accompanying model.  Following instructions on the screen, they will take apart and put together new \"compounds\" in certain key spots on the doll, while watching watching the scientific information regarding that key spot of the digestive system.  The usable parts of the compound will become muscle or another element that is needed for the challenge and the unneeded parts will be excrement.\t\t\t\t\t<br><br><b>Implementation</b><br>\n          \t\t\t\tThe 4x3 foot model is made of a plexiglass front and interior and a masonite backing.  It is used in conjunction with a large touch screen (in an ideal setup) and accompanied by a workspace in which to build the plexiglass food compounds and a poster that introduces the basic ideology of the interactive.\t\t      \t\t<br><br><b>Conclusion</b><br>\n          \t\t\t\tWe learned how to apply learning techniques and museum education methods to physically building the interactive installation.  In addition, our beta testing with children presented a demographic in which we often create projects for, but rarely have a chance in which to interact.  The ability to prototype the project multiple times and have child beta testers significantly informed our work."},{"pitch":"A visual art work composed of links which are represent by functional qr codes.","author":"K TL","title":"Qr Code on Canvas","url":"http://itp.nyu.edu/~ktl243/blog/?p=387","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1336580868_qrcode_on_canvas_sm.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tWhile thinking about the way we digest information I was interested in creating an interactive work on canvas. By printing qr codes on canvas the work requires the viewer to use their mobile phone to access the artwork's full intent. Each qr code leads the viewer to a different appropriation or online appearance of George Seurat's iconic pointillist painting \" Sunday Afternoon on the Island of La Grande Jatte\".<br><br><b>Background</b><br>Art history meets contemporary image digestion.<br><br><b>User Scenario</b><br>\n          \t\t\t\tPeople passing this wall hung work will wonder what the qr codes may lead to and can pull out their smartphone to access the collection of links that I have composed."},{"pitch":"An interactive table that tells stories about the objects on it.","author":"Jack Kalish, Nick Yulman","title":"Secondhand Stories","url":"http://vimeo.com/38339417","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1336592573_thumbnail.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tSecondhand Stories is an interactive installation designed for the context of a flea market or secondhand store. Several curious objects sit on top of the table. When a user picks one up, an outline of the object in light remains on the table, and the name of the previous owner of the object is displayed. The user hears the previous owner of the object tell a story about that object. The light flickers, mimicking the voice of the story teller. When the object is placed back on the table, the audio stops.<br><br><b>User Scenario</b><br>\n          \t\t\t\tA user approaches the table, when they lift an object, they see a name and light projected on the table where that object was sitting, and they hear a voice telling a story about the object. When they put the object down, the audio and light fade out."},{"pitch":"","author":"Maria Rabinovich","title":"Loci","url":"http://locimind.com","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1335221284_small.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tA mobile application that brings an ancient memory technique to our 21st century location aware technologies. <br><br>\nThe Method of Loci is an ancient Roman memory technique in which one uses his or her imagination to place images along a familiar path as a way to remember a sequence of things or ideas. Loci re-imagines this technique within the framework made possible by location aware technologies, to create a learning platform for the urban experience.<br><br>\nHave something you wish to remember? Enter it into the application along a path on a map, and begin the walk. As you reach your locations, what you want to remember, along with dynamically generated mnemonic devices, images, audio, and video, will appear.  <br><br><br><br><br><br><br><b>Background</b><br><br><br><br><br>"},{"pitch":"This project uses the kinect to track the hand motions of a conductor to control audio and visual elements.","author":"Byung Han Lim, Nicholas Johnson","title":"Dreaming Maestro","url":"http://itp.nyu.edu/shows/spring2012/dreaming-maestro/","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1336667868_conductorpainting.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tDigital Conducting focuses on the mechanics of the human body specifically pertaining to the hand and arm motions used by a conductor.  Using the kinect camera and the programming environment MaxMSP/Jitter, a users movements and gestures are closely tracked providing accurate beat tracking, tempo tracking and conducting cues which are then used to create and trigger audio and visual elements.  The installation allows users the opportunity to experience the correlation between the motion of a conductor and the sound being produced by these movements."},{"pitch":"A set of gamelan instruments that fit on your laptop sleeve so you can practice gamelan music on-the-go.","author":"Antonius Oktaviano Wiriadjaja","title":"Gamelan Sampul","url":"http://gamelan.antoni.us","image":"http://itp.nyu.edu/shows/spring2012/wp-content/uploads/Spring-Show-2012/images/1336567978_laptopgamel.jpg","more":"<!--<div class=\"entry\">-->\n\t\t\n\t\t\tAs part of my thesis, I built a portable practice gamelan instrument embedded in my laptop sleeve so I could rehearse my music anywhere that I carry my laptop. The Gamelan Sampul, which means 'envelope gamelan', features touch-sensitive pads that trigger noises that one would usually hear from real Javanese gamelan instruments. It also acts as a recording device, allowing users to keep an archive of their own and other players' musical style. I will also present a set of new sensors currently in development that give haptic and visual feedback as well as more accurate sensing of when it is hit."}];
